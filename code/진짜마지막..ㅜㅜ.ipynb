{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 191108\n",
    "- outlier drop 해보기\n",
    "- Missing value ; numeric -> median, cat -> mode로 해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Feature/Data Transformation\n",
    "- Outlier\n",
    "- New Features\n",
    "\n",
    "## 2.Missing Data Imputation\n",
    "- Numeric columns: median\n",
    "- Categorical columns: mode\n",
    "\n",
    "## 3.Dimensionality Reduction\n",
    "- Features with Bad or Constant Data\n",
    "- Multicollinearity and Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import datetime\n",
    "import scipy as sp\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrix\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import utils.preprocessing as pp \n",
    "import utils.correlation as cr\n",
    "import utils.statsmodel_helper as st\n",
    "\n",
    "df_macro = pd.read_csv('../code/data/macro.csv', parse_dates=['timestamp'])\n",
    "df_train = pd.read_csv('../code/data/train.csv', index_col=0, parse_dates=['timestamp'])\n",
    "df_test = pd.read_csv('../code/data/test.csv', index_col=0, parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Feature/Data Transformation\n",
    "- Outlier\n",
    "- New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop data with extremely big price #\n",
    "df_train = df_train.drop([2121]) \n",
    "\n",
    "# Drop outliers with proper value #\n",
    "df_train.loc[df_train.state == 33, 'state'] = 3\n",
    "df_train.loc[df_train['life_sq'] > 1000,     'life_sq']       = np.median(df_train['life_sq'].dropna())\n",
    "df_train.loc[df_train['kitch_sq'] > 250,     'kitch_sq']      = np.median(df_train['kitch_sq'].dropna())\n",
    "df_train.loc[df_train['num_room'] > 6,       'num_room']      = np.median(df_train['num_room'].dropna())\n",
    "df_train.loc[df_train['build_year'] > 2017,  'build_year']    = np.median(df_train['build_year'].dropna())\n",
    "df_train.loc[df_train['build_year'] < 1800,  'build_year']    = np.median(df_train['build_year'].dropna())\n",
    "df_train.loc[df_train['floor'] > 50,         'floor']         = np.median(df_train['floor'].dropna())\n",
    "df_train.loc[df_train['max_floor'] > 60,     'max_floor']     = np.median(df_train['max_floor'].dropna())\n",
    "df_train.loc[df_train.full_sq == 0, 'full_sq'] = 50\n",
    "df_train = df_train[df_train.price_doc/df_train.full_sq <= 600000]\n",
    "df_train = df_train[df_train.price_doc/df_train.full_sq >= 10000]\n",
    "\n",
    "df_test.loc[df_test['life_sq'] > 1000,     'life_sq']       = np.median(df_test['life_sq'].dropna())\n",
    "df_test.loc[df_test['kitch_sq'] > 250,     'kitch_sq']      = np.median(df_test['kitch_sq'].dropna())\n",
    "df_test.loc[df_test['num_room'] > 6,       'num_room']      = np.median(df_test['num_room'].dropna())\n",
    "df_test.loc[df_test['build_year'] > 2017,  'build_year']    = np.median(df_test['build_year'].dropna())\n",
    "df_test.loc[df_test['build_year'] < 1800,  'build_year']    = np.median(df_test['build_year'].dropna())\n",
    "df_test.loc[df_test['floor'] > 50,         'floor']         = np.median(df_test['floor'].dropna())\n",
    "df_test.loc[df_test['max_floor'] > 60,     'max_floor']     = np.median(df_test['max_floor'].dropna())\n",
    "df_test.loc[df_test.full_sq == 0, 'full_sq'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.index[df['line_race'] == 0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and day of week #\n",
    "df_train['month'] = df_train.timestamp.dt.month\n",
    "df_train['dow'] = df_train.timestamp.dt.dayofweek\n",
    "\n",
    "df_test['month'] = df_test.timestamp.dt.month\n",
    "df_test['dow'] = df_test.timestamp.dt.dayofweek\n",
    "\n",
    "# Create new features that might help #\n",
    "df_train['rel_floor'] = df_train['floor'] / df_train['max_floor'].astype(float)\n",
    "df_train['rel_kitch_sq'] = df_train['kitch_sq'] / df_train['full_sq'].astype(float)\n",
    "\n",
    "df_test['rel_floor'] = df_test['floor'] / df_test['max_floor'].astype(float)\n",
    "df_test['rel_kitch_sq'] = df_test['kitch_sq'] / df_test['full_sq'].astype(float)\n",
    "\n",
    "df_train.apartment_name=df_train.sub_area + df_train['metro_km_avto'].astype(str)\n",
    "df_test.apartment_name=df_test.sub_area + df_train['metro_km_avto'].astype(str)\n",
    "\n",
    "df_train['room_size'] = df_train['life_sq'] / df_train['num_room'].astype(float)\n",
    "df_test['room_size'] = df_test['life_sq'] / df_test['num_room'].astype(float)\n",
    "\n",
    "# Average price corresponding to sub_area and ID_* #\n",
    "id_features = ['ID_metro',\n",
    "    'ID_railroad_station_walk', \\\n",
    "    'ID_big_road1', \\\n",
    "    'ID_big_road2', \\\n",
    "    'ID_railroad_terminal', \\\n",
    "    'ID_bus_terminal']\n",
    "\n",
    "for id_f in id_features:\n",
    "    df_test['avg_price_' + id_f] = 0.0\n",
    "    for val in df_test[id_f].unique():\n",
    "        if val == 171 and id_f == 'ID_metro':\n",
    "            df_test.loc[df_test.ID_metro == 171, 'avg_price_ID_metro'] = df_train[df_train.ID_metro == 170]['price_doc'].mean()\n",
    "            continue\n",
    "        if val == 132 and id_f == 'ID_railroad_station_walk':\n",
    "            df_test.loc[df_test.ID_railroad_station_walk == 132, 'avg_price_ID_railroad_station_walk'] = df_train[df_train.ID_railroad_station_walk == 131]['price_doc'].mean()\n",
    "            continue\n",
    "        if val == 121 and id_f == 'ID_railroad_station_walk':\n",
    "            df_test.loc[df_test.ID_railroad_station_walk == 122, 'avg_price_ID_railroad_station_walk'] = df_train[df_train.ID_railroad_station_walk == 131]['price_doc'].mean()\n",
    "            continue\n",
    "        avg = df_train[df_train[id_f] == val]['price_doc'].mean()\n",
    "        df_test.loc[df_test[id_f] == val, 'avg_price_' + id_f] = avg\n",
    "    del df_test[id_f]\n",
    "    \n",
    "for id_f in id_features:\n",
    "    df_train['avg_price_' + id_f] = 0.0\n",
    "    for val in df_train[id_f].unique():\n",
    "        avg = df_train[df_train[id_f] == val]['price_doc'].mean()\n",
    "        df_train.loc[df_train[id_f] == val, 'avg_price_' + id_f] = avg\n",
    "    del df_train[id_f]\n",
    "    \n",
    "cols = list(df_train.columns.values)\n",
    "cols.pop(cols.index('price_doc'))\n",
    "df_train = df_train[cols + ['price_doc']]\n",
    "\n",
    "\n",
    "df_test['avg_price_sub_area'] = 0.0\n",
    "df_train['avg_price_sub_area'] = 0.0\n",
    "for subarea in df_train['sub_area'].unique():\n",
    "    avg = df_train[df_train['sub_area'] == subarea]['price_doc'].mean()\n",
    "    df_train.loc[df_train['sub_area'] == subarea, 'avg_price_sub_area'] = avg\n",
    "    df_test.loc[df_test['sub_area'] == subarea, 'avg_price_sub_area'] = avg\n",
    "del df_train['sub_area']\n",
    "del df_test['sub_area']\n",
    "\n",
    "\n",
    "# Add the Macro Feature #\n",
    "usdrub_pairs = dict(zip(list(df_macro['timestamp']), list(df_macro['usdrub'])))\n",
    "\n",
    "df_train['timestamp'].replace(usdrub_pairs,inplace=True)\n",
    "df_test['timestamp'].replace(usdrub_pairs,inplace=True)\n",
    "\n",
    "df_train.rename(columns={'timestamp' : 'usdrub'}, inplace=True)\n",
    "df_test.rename(columns={'timestamp' : 'usdrub'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Missing Data Imputation\n",
    "- Numeric columns: median\n",
    "- Categorical columns: mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.find_missing_data_columns(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.find_missing_data_columns(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_train._get_numeric_data()[df_train._get_numeric_data() < 0] = 0\n",
    "df_test._get_numeric_data()[df_test._get_numeric_data() < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric #\n",
    "for col in df_train._get_numeric_data().columns[df_train._get_numeric_data().columns.isnull().any()].tolist():\n",
    "    df_train[col].fillna(df_train[col].median(), inplace=True)\n",
    "for col in df_test._get_numeric_data().columns[df_test._get_numeric_data().columns.isnull().any()].tolist():\n",
    "    df_test[col].fillna(df_train[col].median(), inplace=True)\n",
    "\n",
    "# categorical #\n",
    "for col in df_train.columns[df_train.isnull().any()].tolist():\n",
    "    df_train[col].fillna(df_train[col].value_counts().index[0], inplace=True)\n",
    "for col in df_test.columns[df_test.isnull().any()].tolist():\n",
    "    df_test[col].fillna(df_train[col].value_counts().index[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.find_missing_data_columns(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.find_missing_data_columns(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_col = list(data.columns) # list of all columns name \n",
    "#train_df = data.dropna()   # drop null\n",
    "train_df = data\n",
    "train_df_catcol=train_df.dtypes[train_df.dtypes == 'object']  #list of categorical columns name\n",
    "train_df_cat = train_df[train_df_catcol.index]   # df with only categorical columns\n",
    "train_df_realcol = [e for e in all_col if e not in list(train_df_catcol.index)]   # list of non-categorical columns name (only real date type)\n",
    "train_df_real= train_df[train_df_realcol] #df with only real data type columns\n",
    "#fill na with median for real data df\n",
    "train_df_real.fillna(train_df_real.median(),inplace=True)\n",
    "train_df_cat.fillna(train_df_cat.mode(), inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
