{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mutils\u001b[00m\n",
      "\n",
      "0 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "!tree utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch utils/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mutils\u001b[00m\n",
      "└── __init__.py\n",
      "\n",
      "0 directories, 1 file\n"
     ]
    }
   ],
   "source": [
    "!tree utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/preprocessing.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils.correlation as corr\n",
    "import statsmodels.api as sm\n",
    "import utils.statsmodel_helper as smh\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def merge(df_1, df_2, on_col):\n",
    "    df_tm = pd.merge(df_1, df_2, on=[on_col, on_col])\n",
    "    df_tm_cols = df_tm.columns.tolist()\n",
    "    df_tm_cols = df_tm_cols[:290] + df_tm_cols[291:] + [df_tm_cols[290]]\n",
    "    df_tm = df_tm[df_tm_cols]\n",
    "    return df_tm\n",
    "\n",
    "def clean_data(df):\n",
    "    # build_year 1500이전 nan으로\n",
    "    df.loc[df.build_year < 1500, 'build_year'] = np.nan\n",
    "    df.loc[df.build_year > 2016, 'build_year'] = np.nan\n",
    "    \n",
    "    # floor가 0이면 nan으로\n",
    "    df.loc[df.floor==0, 'floor'] = np.nan\n",
    "    \n",
    "    # max_floor가 0이면 nan으로\n",
    "    df.loc[df.max_floor==0, 'max_floor'] = np.nan\n",
    "    \n",
    "    # max_floor가 floor보다 크면 nan으로\n",
    "    df.loc[df.floor>df.max_floor, 'max_floor'] = np.nan\n",
    "    \n",
    "    # full_sq, life_sq, kitch_sq가 0이면 nan으로\n",
    "    df.loc[df.full_sq==0, 'full_sq'] = np.nan\n",
    "    df.loc[df.life_sq==0, 'life_sq'] = np.nan\n",
    "    df.loc[df.kitch_sq==0, 'kitch_sq'] = np.nan\n",
    "    \n",
    "    # full_sq가 life_sq보다 작으면 nan으로\n",
    "    df.loc[df.life_sq>df.full_sq, 'life_sq'] = np.nan\n",
    "    \n",
    "    # kitch_sq가 life_sq보다 크면 nan으로\n",
    "    df.loc[df.kitch_sq>df.life_sq, 'kitch_sq'] = np.nan\n",
    "    \n",
    "    df.loc[df.state == 33, 'state'] = 3\n",
    "    \n",
    "    df.loc[df.full_sq > 210, 'full_sq'] == np.nan\n",
    "    df.loc[df.full_sq > 200, 'full_sq'] == np.nan    \n",
    "\n",
    "    df.loc[df.num_room < 0, 'num_room'] = np.nan\n",
    "    \n",
    "    df['material'].fillna(0, inplace=True)\n",
    "    \n",
    "    # 이상한 숫자값들 45,34 ...\n",
    "    if 'modern_education_share' in df: del df['modern_education_share']\n",
    "    if 'old_education_build_share' in df: del df['old_education_build_share']\n",
    "    if 'child_on_acc_pre_school' in df: del df['child_on_acc_pre_school']\n",
    "        \n",
    "    consts = [col for col in df.columns if len(df[col].value_counts().index) == 1]\n",
    "    for const in consts:\n",
    "        del df[const]\n",
    "        \n",
    "    df = df.replace(['no data'], ['nodata'])\n",
    "    \n",
    "#     # 뉴머릭한 카테고리컬 독립변수들인데 유니크값이 너무 많아서 없앤다.\n",
    "#     del df['ID_railroad_station_walk']\n",
    "#     del df['ID_railroad_station_avto']\n",
    "#     del df['ID_big_road1']\n",
    "#     del df['ID_big_road2']\n",
    "#     del df['ID_railroad_terminal']\n",
    "#     del df['ID_bus_terminal']\n",
    "#     del df['ID_metro']\n",
    "#     # too many dummy variables\n",
    "#     del df['sub_area']\n",
    "    \n",
    "#     50% 이상 미싱 데이터가 있으면 없애버린다\n",
    "    if 'provision_retail_space_sqm' in df: del df['provision_retail_space_sqm']\n",
    "    if 'theaters_viewers_per_1000_cap' in df: del df['theaters_viewers_per_1000_cap']\n",
    "    if 'museum_visitis_per_100_cap' in df: del df['museum_visitis_per_100_cap']\n",
    "    \n",
    "    # material은 카테고리\n",
    "#     df['material'] = df['material'].astype(np.str, copy=False)\n",
    "#     df['material'] = df['material'].replace([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0], ['a', 'b', 'c', 'd', 'e', 'f', 'e'])\n",
    "#     return df\n",
    "\n",
    "\n",
    "def col_renames(df):\n",
    "    df.rename(columns={'build_count_1921-1945': 'build_count_1921_1945', 'build_count_1946-1970': 'build_count_1946_1970', 'build_count_1971-1995': 'build_count_1971_1995'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def del_many_unique(df):\n",
    " # 뉴머릭한 카테고리컬 독립변수들인데 유니크값이 너무 많아서 없앤다.\n",
    "    del df['ID_railroad_station_walk']\n",
    "    del df['ID_railroad_station_avto']\n",
    "    del df['ID_big_road1']\n",
    "    del df['ID_big_road2']\n",
    "    del df['ID_railroad_terminal']\n",
    "    del df['ID_bus_terminal']\n",
    "    del df['ID_metro']\n",
    "    # too many dummy variables\n",
    "    del df['sub_area']\n",
    "    del df['prom_part_3000']\n",
    "    return df\n",
    "\n",
    "def categorize(df):\n",
    "    df['material'] = df['material'].astype(np.object, copy=False)\n",
    "    df['material'] = df['material'].replace([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0], ['a', 'b', 'c', 'd', 'e', 'f', 'e'])\n",
    "# def find_missing_data_columns(df):\n",
    "#     missing_df = df.isnull().sum(axis=0).reset_index()\n",
    "#     missing_df.columns = ['missing_column', 'missing_count']\n",
    "#     missing_df = missing_df.loc[missing_df['missing_count'] > 0]\n",
    "#     return missing_df\n",
    "\n",
    "\n",
    "def impute_num_mode(df):\n",
    "    for col in df._get_numeric_data().columns[df._get_numeric_data().isnull().any()]:\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "def imput_cat_mode(df):\n",
    "    for col in df.column[df.isnull().any()].tolist():\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "        \n",
    "def apply_log(df, numeric_cols):\n",
    "    for col in numeric_cols:\n",
    "        min_val = min(df[col].value_counts().index)\n",
    "        if min_val < 0:\n",
    "            df[col] -= min_val\n",
    "            df[col] += 1\n",
    "        else:\n",
    "            df[col] += 1\n",
    "    df[numeric_cols].apply(np.log)\n",
    "\n",
    "def scale_up_positive(df, numeric_cols):\n",
    "    for col in numeric_cols:\n",
    "        min_val = min(df[col].value_counts().index)\n",
    "        if min_val < 0:\n",
    "            df[col] -= min_val\n",
    "            df[col] += 1\n",
    "        else:\n",
    "            df[col] += 1\n",
    "            \n",
    "def remove_outliers(df, formula, repeat=1):\n",
    "    result = None\n",
    "    for i in range(repeat):\n",
    "        model = sm.OLS.from_formula(formula, data=df)\n",
    "        result = model.fit()\n",
    "        influence = result.get_influence()\n",
    "        distances, pvalues = influence.cooks_distance\n",
    "        threshold = 4/(len(distances) - len(df.columns.drop(['_price_doc']))-1)\n",
    "        outliers = [idx for idx, d in enumerate(distances) if d > threshold]\n",
    "        df.drop(df.index[outliers], inplace=True)\n",
    "    return df, model, result\n",
    "\n",
    "def remove_features_by_vif(df):\n",
    "    features_to_remove = [ \n",
    "        'raion_popul', \\\n",
    "        'preschool_education_centers_raion', \\\n",
    "        'school_education_centers_raion', \\\n",
    "        'sport_objects_raion', \\\n",
    "        'office_raion', \\\n",
    "        'young_all', \\\n",
    "        'work_all', \\\n",
    "        'ekder_all', \\\n",
    "        '0_17_all', \\\n",
    "        'raion_build_count_with_material_info', \\\n",
    "        'raion_build_count_with_builddate_info', \\\n",
    "        'build_count_1946-1970', \\\n",
    "        'metro_min_avto', \\\n",
    "        'metro_km_avto', \\\n",
    "        'metro_min_walk', \\\n",
    "        'school_km', \\\n",
    "        'park_km', \\\n",
    "        'railroad_station_walk_min', \\\n",
    "        'railroad_station_avto_min', \\\n",
    "        'ttk_km', \\\n",
    "        'sadovoe_km', \\\n",
    "        'bulvar_ring_km', \\\n",
    "        'kremlin_km', \\\n",
    "        'zd_vokzaly_avto_km', \\\n",
    "        'bus_terminal_avto_km', \\\n",
    "        'oil_chemistry_km', \\\n",
    "        'nuclear_reactor_km', \\\n",
    "        'radiation_km', \\\n",
    "        'power_transmission_line_km', \\\n",
    "        'thermal_power_plant_km', \\\n",
    "        'ts_km', \\\n",
    "        'swim_pool_km', \\\n",
    "        'ice_rink_km', \\\n",
    "        'stadium_km', \\\n",
    "        'basketball_km', \\\n",
    "        'detention_facility_km', \\\n",
    "        'public_healthcare_km', \\\n",
    "        'university_km', \\\n",
    "        'workplaces_km', \\\n",
    "        'shopping_centers_km', \\\n",
    "        'preschool_km', \\\n",
    "        'big_church_km', \\\n",
    "        'mosque_km', \\\n",
    "        'theater_km', \\\n",
    "        'museum_km', \\\n",
    "        'exhibition_km', \\\n",
    "        'cafe_count_500', \\\n",
    "        'cafe_sum_500_min_price_avg', \\\n",
    "        'cafe_avg_price_500', \\\n",
    "        'office_count_1000', \\\n",
    "        'cafe_count_1000', \\\n",
    "        'cafe_sum_1000_min_price_avg', \\\n",
    "        'cafe_sum_1000_max_price_avg', \\\n",
    "        'cafe_avg_price_1000', \\\n",
    "        'cafe_count_1000_na_price', \\\n",
    "        'cafe_count_1000_price_1000', \\\n",
    "        'cafe_count_1000_price_1500', \\\n",
    "        'office_count_1500', \\\n",
    "        'cafe_count_1500', \\\n",
    "        'cafe_sum_1500_max_price_avg', \\\n",
    "        'cafe_avg_price_1500', \\\n",
    "        'cafe_count_1500_na_price', \\\n",
    "        'cafe_count_1500_price_500', \\\n",
    "        'cafe_count_1500_price_1000', \\\n",
    "        'cafe_count_1500_price_1500', \\\n",
    "        'cafe_count_1500_price_2500', \\\n",
    "        'cafe_count_1500_price_high', \\\n",
    "        'leisure_count_1500', \\\n",
    "        'sport_count_1500', \\\n",
    "        'green_part_2000', \\\n",
    "        'office_count_2000', \\\n",
    "        'office_sqm_2000', \\\n",
    "        'trc_count_2000', \\\n",
    "        'cafe_count_2000', \\\n",
    "        'cafe_sum_2000_min_price_avg', \\\n",
    "        'cafe_sum_2000_max_price_avg', \\\n",
    "        'cafe_avg_price_2000', \\\n",
    "        'cafe_count_2000_na_price', \\\n",
    "        'cafe_count_2000_price_500', \\\n",
    "        'cafe_count_2000_price_1000', \\\n",
    "        'cafe_count_2000_price_1500', \\\n",
    "        'cafe_count_2000_price_2500', \\\n",
    "        'cafe_count_2000_price_high', \\\n",
    "        'sport_count_2000', \\\n",
    "        'green_part_3000', \\\n",
    "        'office_count_3000', \\\n",
    "        'office_sqm_3000', \\\n",
    "        'trc_count_3000', \\\n",
    "        'cafe_count_3000', \\\n",
    "        'cafe_count_3000_na_price', \\\n",
    "        'cafe_count_3000_price_500', \\\n",
    "        'cafe_count_3000_price_1000', \\\n",
    "        'cafe_count_3000_price_1500', \\\n",
    "        'cafe_count_3000_price_2500', \\\n",
    "        'cafe_count_3000_price_4000', \\\n",
    "        'cafe_count_3000_price_high', \\\n",
    "        'big_church_count_3000', \\\n",
    "        'church_count_3000', \\\n",
    "        'leisure_count_3000', \\\n",
    "        'sport_count_3000', \\\n",
    "        'green_part_5000',\\\n",
    "        'office_count_5000',\\\n",
    "        'office_sqm_5000',\\\n",
    "        'trc_count_5000',\\\n",
    "        'trc_sqm_5000',\\\n",
    "        'cafe_count_5000',\\\n",
    "        'cafe_count_5000_na_price', \\\n",
    "        'cafe_count_5000_price_500', \\\n",
    "        'cafe_count_5000_price_1000', \\\n",
    "        'cafe_count_5000_price_1500', \\\n",
    "        'cafe_count_5000_price_2500', \\\n",
    "        'cafe_count_5000_price_4000', \\\n",
    "        'cafe_count_5000_price_high', \\\n",
    "        'big_church_count_5000', \\\n",
    "        'church_count_5000', \\\n",
    "        'leisure_count_5000', \\\n",
    "        'sport_count_5000', \\\n",
    "        'market_count_5000', \\\n",
    "        'avg_price_ID_metro', \\\n",
    "        'avg_price_ID_railroad_station_walk', \\\n",
    "        'avg_price_ID_big_road1', \\\n",
    "        'avg_price_ID_big_road2', \\\n",
    "        'avg_price_ID_railroad_terminal', \\\n",
    "        'avg_price_ID_bus_terminal', \\\n",
    "        'avg_price_sub_area' \\\n",
    "    ]\n",
    "    for f in features_to_remove:\n",
    "        if f in df_train:\n",
    "            del df_train[f]\n",
    "\n",
    "def scale_up_positive(df, numeric_cols):\n",
    "    for col in numeric_cols:\n",
    "        min_val = min(df[col].value_counts().index)\n",
    "        if min_val < 0:\n",
    "            df[col] -= min_val\n",
    "            df[col] += 1\n",
    "        else:\n",
    "            df[col] += 1\n",
    "\n",
    "def remove_features_by_high_corr(df):            \n",
    "    features_to_remove = [\n",
    "        'children_preschool', 'children_school', 'male_f', \\\n",
    "        'female_f', 'young_male', 'young_female', 'work_male', \\\n",
    "        'work_female', 'ekder_male', 'ekder_female', '16_29_all',\\\n",
    "        '0_6_all', '0_6_male', '0_6_female',\\\n",
    "        '7_14_all', '7_14_male', '7_14_female', '0_17_male', '0_17_female',\\\n",
    "        '16_29_male', '16_29_female', '0_13_all', '0_13_male', '0_13_female',\\\n",
    "        'metro_km_walk', 'railroad_station_walk_km',\\\n",
    "        'railroad_station_avto_km', 'public_transport_station_km' \\\n",
    "    ]\n",
    "    for f in features_to_remove:\n",
    "        del df[f]\n",
    "        \n",
    "def find_missing_data_columns(df):\n",
    "    missing_df = df.isnull().sum(axis=0).reset_index()\n",
    "    missing_df.columns = ['missing_column', 'missing_count']\n",
    "    missing_df = missing_df.loc[missing_df['missing_count'] > 0]\n",
    "    return missing_df\n",
    "\n",
    "def imput_by_interpolate(df):\n",
    "    missing_df = find_missing_data_columns(df)\n",
    "    for col in missing_df['missing_column']:\n",
    "        df[col] = df[col].interpolate(mathod='linear')\n",
    "    return df\n",
    "\n",
    "def impute_by_regression(df, repeat, corr_thresh):\n",
    "    for i in range(repeat):\n",
    "        pairs = []\n",
    "        missing_df = find_missing_data_columns(df)\n",
    "        for missing_col in missing_df['missing_column']:\n",
    "            if not np.issubdtype(df[missing_col], np.number) : continue\n",
    "            corrs = [ (missing_col, c, abs(df[missing_col].corr(df[c]))) for c in df._get_numeric_data().columns if c != missing_col ]\n",
    "            corrs.sort(key=lambda item : item[2], reverse=True)\n",
    "            for item in corrs:\n",
    "                if item[2] >= corr_thresh:\n",
    "                    pairs.append(item)\n",
    "                else:\n",
    "                    break\n",
    "        df_nan_col_with_high_corr_col = pd.DataFrame(pairs, columns=['missing_col', 'highest corr with', 'corr'])\n",
    "        for row in df_nan_col_with_high_corr_col.iterrows():\n",
    "            if df[row[1][0]].isnull().sum() <= 0 : continue\n",
    "            nan_col = row[1][0]\n",
    "            high_corr_col = row[1][1]\n",
    "            corr = row[1][1]\n",
    "            \n",
    "            df_temp = pd.DataFrame(df[[high_corr_col, nan_col]], columns=[high_corr_col, nan_col])\n",
    "            df_temp = df_temp.dropna()\n",
    "            \n",
    "            df_temp = sm.add_constant(df_temp)\n",
    "            X = df_temp.values[:, :2]\n",
    "            y = df_temp.values[:, 2]\n",
    "            result = sm.OLS(y, X).fit()\n",
    "            \n",
    "            dfX = sm.add_constant(df[high_corr_col])\n",
    "            predicted = result.predict(dfX)\n",
    "            \n",
    "            df = pd.merge(df, predicted.to_frame('predicted'), left_index=True, right_index=True)\n",
    "            df[nan_col].fillna(df['predicted'], inplace=True)\n",
    "            del df['predicted']\n",
    "    return df\n",
    "\n",
    "def impute_by_regression2(df, repeat, corr_thresh):\n",
    "    for i in range(repeat):\n",
    "        pairs = []\n",
    "        missing_df = find_missing_data_columns(df)\n",
    "        for missing_col in missing_df['missing_column']:\n",
    "            if not np.issubdtype(df[missing_col], np.number) : continue\n",
    "            corrs = [ (missing_col, c, abs(df[missing_col].corr(df[c]))) for c in df._get_numeric_data().columns if c != missing_col ]\n",
    "            corrs.sort(key=lambda item : item[2], reverse=True)\n",
    "            for item in corrs:\n",
    "                if item[2] >= corr_thresh:\n",
    "                    pairs.append(item)\n",
    "                else:\n",
    "                    break\n",
    "        df_nan_col_with_high_corr_col = pd.DataFrame(pairs, columns=['missing_col', 'highest corr with', 'corr'])\n",
    "        for row in df_nan_col_with_high_corr_col.iterrows():\n",
    "            if df[row[1][0]].isnull().sum() <= 0 : continue\n",
    "            nan_col = row[1][0]\n",
    "            high_corr_col = row[1][1]\n",
    "            corr = row[1][1]\n",
    "            \n",
    "            df_temp = pd.DataFrame(df[[high_corr_col, nan_col]], columns=[high_corr_col, nan_col])\n",
    "            df_temp = df_temp.dropna()\n",
    "            \n",
    "            df_temp = sm.add_constant(df_temp)\n",
    "            X = df_temp.values[:, :2]\n",
    "            y = df_temp.values[:, 1]\n",
    "            result = sm.OLS(y, X).fit()\n",
    "            \n",
    "            dfX = sm.add_constant(df[high_corr_col])\n",
    "            predicted = result.predict(dfX)\n",
    "            \n",
    "            df = pd.merge(df, predicted.to_frame('predicted'), left_index=True, right_index=True)\n",
    "            df[nan_col].fillna(df['predicted'], inplace=True)\n",
    "            del df['predicted']\n",
    "    return df\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils/feature_selection.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/feature_selection.py\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "def by_f_test(df, formula, repeat=10, log_dv = True):\n",
    "    result = None\n",
    "    selected_ivs = []\n",
    "    for i in range(repeat):\n",
    "        model = sm.OLS.from_formula(formula, data=df)\n",
    "        result = model.fit()\n",
    "        anova = sm.stats.anova_lm(result, typ=2)\n",
    "        selected_ivs = [iv[0] for iv in anova.iterrows() if iv[1][3] < 0.05]\n",
    "        if len(selected_ivs) >= 0:\n",
    "            if log_dv == True:  \n",
    "                formula = 'np.log(_price_doc) ~ ' + ' + '.join(selected_ivs)\n",
    "            else:\n",
    "                formula = '_price_doc ~ ' + ' + '.join(selected_ivs)\n",
    "        else:\n",
    "            return result, selected_ivs\n",
    "    return result, selected_ivs,  formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils/statsmodel_helper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/statsmodel_helper.py\n",
    "\n",
    "import statsmodels as sm\n",
    "\n",
    "def make_statsmodels_ols_formula(numeric_ivs, categorical_ivs, dv, log_vs=[], degree=1, scale=True):\n",
    "\n",
    "\n",
    "    if len(log_vs) > 0:\n",
    "        numeric_ivs = [\"np.log({})\".format(iv) if iv in log_vs else iv for iv in numeric_ivs ]\n",
    "\n",
    "    polynomials = []\n",
    "    if degree > 1:\n",
    "        for i in range(2, degree + 1):\n",
    "            if scale:\n",
    "                polynomials = list(map(lambda iv: 'scale(I({}**{}))'.format(iv, i), numeric_ivs))\n",
    "            else:\n",
    "                polynomials = list(map(lambda iv: 'I({}**{})'.format(iv, i), numeric_ivs))\n",
    "    \n",
    "    if scale:\n",
    "        numeric_ivs = [\"scale({})\".format(iv) if scale else iv for iv in numeric_ivs ]\n",
    "\n",
    "    formula = ''\n",
    "    if dv in log_vs:\n",
    "        formula = 'np.log({}) ~ '.format(dv)\n",
    "    else:\n",
    "        formula = '{} ~ '.format(dv)\n",
    "    \n",
    "\n",
    "    if len(categorical_ivs) > 0:\n",
    "        if len(numeric_ivs) > 0:\n",
    "            formula += \" + \".join(list(map(lambda iv: 'C({})'.format(iv), categorical_ivs)))\n",
    "        else:\n",
    "            formula += \" + \".join(list(map(lambda iv: 'C({})-1'.format(iv), categorical_ivs)))\n",
    "    \n",
    "    if len(polynomials) > 0:\n",
    "        if len(categorical_ivs) > 0:\n",
    "            return  formula + \" + \" + \" + \".join(numeric_ivs) + \" + \" + \" + \".join(polynomials)\n",
    "        else:\n",
    "            return  formula + \" + \".join(numeric_ivs) + \" + \" + \" + \".join(polynomials)\n",
    "    else:\n",
    "        if len(categorical_ivs) > 0:\n",
    "            return formula + \" + \" + \" + \".join(numeric_ivs)\n",
    "        else:\n",
    "            return formula + \" + \".join(numeric_ivs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils/error_calculator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/error_calculator.py\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "  \n",
    "def rmsle(pred, real):\n",
    "\t# 넘파이로 배열 형태로 바꿔줌.  \n",
    "    predicted_values = pred.values\n",
    "    actual_values = real.values\n",
    "    \n",
    "  # 예측값과 실제 값에 1을 더하고 로그를 씌어줌 \n",
    "    log_predict = np.log(predicted_values + 1)\n",
    "    log_actual = np.log(actual_values + 1)\n",
    "    \n",
    "  # 위에서 계산한 예측값에서 실측값을 빼주고 제곱해줌\n",
    "    difference = log_predict - log_actual\n",
    "    difference = np.square(difference)\n",
    "    \n",
    "  # 평균을 냄\n",
    "    mean_difference = difference.mean()\n",
    "    \n",
    "  # 다시 루트를 씌움\n",
    "    score = np.sqrt(mean_difference)  \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils/correlation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/correlation.py\n",
    "import numpy as np\n",
    "\n",
    "def pick_highly_correlated_features(df, columns, min_corr):\n",
    "    pairs = []\n",
    "    for col in columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            continue\n",
    "        corrs = [(col, c, abs(df[col].corr(df[c]))) for c in df.columns.values.tolist() if c != col]\n",
    "        corrs.sort(key=lambda item: item[2], reverse=True)\n",
    "        for item in corrs:\n",
    "            if item[2] > min_corr:\n",
    "                pairs.append(item)\n",
    "            else:\n",
    "                break\n",
    "    return pd.DataFrame(pairs, columns=['missing_col', 'highest corr with', 'corr'])\n",
    "\n",
    "def pick_highly_correlated_IVs(df, target_col, min_corr, min_unique_values = 0):\n",
    "    if not np.issubdtype(df[target_col].dtype, np.number):\n",
    "        Exception('{}은 numeric data가 아닙니다.'.format(target_col))\n",
    "    # if len(df[col].value_counts().index) < min_unique_values:\n",
    "    #     Exception('{}로 상관관계를 계산하기에는 유니크한 값이 너무 작습니다.'.format(col))\n",
    "\n",
    "    corrs = []\n",
    "    for col in df._get_numeric_data().drop(target_col, axis=1).columns:\n",
    "        if len(df[col].value_counts().index) < min_unique_values: continue\n",
    "        corr = abs(df[target_col].corr(df[col]))\n",
    "        if corr > min_corr:\n",
    "            corrs.append((col, corr))\n",
    "    \n",
    "    return corrs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mutils\u001b[00m\n",
      "├── correlation.py\n",
      "├── error_calculator.py\n",
      "├── feature_selection.py\n",
      "├── __init__.py\n",
      "├── preprocessing.py\n",
      "└── statsmodel_helper.py\n",
      "\n",
      "0 directories, 6 files\n"
     ]
    }
   ],
   "source": [
    "!tree utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
