{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with StatsModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ordinary Least Square\n",
    "- Column Names\n",
    "- Log Transformation\n",
    "- Condition Number\n",
    "- Standard Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dimensionality Reduction\n",
    "- ANOVA\n",
    "- F-test and Feature Influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Outlier\n",
    "- Cook's Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regularization\n",
    "- Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Diagnosis of Regression\n",
    "- Residual Normality Test\n",
    "- Partial Regression Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cross Validatoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Test\n",
    "- score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# pd.options.mode.chained_assignment = None\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# import warnings\n",
    "# import sys\n",
    "# import datetime\n",
    "# import scipy as sp\n",
    "# import statsmodels.stats.api as sms\n",
    "# import statsmodels.api as sm\n",
    "# from patsy import dmatrix\n",
    "# from sklearn.base import BaseEstimator, RegressorMixin\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import r2_score\n",
    "# import utils.statsmodel_helper as sh\n",
    "# import utils.feature_selection as fs\n",
    "# import utils.preprocessing as pp\n",
    "# import utils.error_calculator as ec\n",
    "\n",
    "\n",
    "# class SMWrapper(BaseEstimator, RegressorMixin):\n",
    "#     \"\"\" A universal sklearn-style wrapper for statsmodels regressors \"\"\"\n",
    "#     def __init__(self, model_class, fit_intercept=True):\n",
    "#         self.model_class = model_class\n",
    "#         self.fit_intercept = fit_intercept\n",
    "#     def fit(self, X, y):\n",
    "#         if self.fit_intercept:\n",
    "#             X = sm.add_constant(X)\n",
    "#         self.model_ = self.model_class(y, X)\n",
    "#         self.results_ = self.model_.fit()\n",
    "#     def predict(self, X):\n",
    "#         if self.fit_intercept:\n",
    "#             X = sm.add_constant(X)\n",
    "#         return self.results_.predict(X)\n",
    "\n",
    "\n",
    "\n",
    "# sys.setrecursionlimit(1500)\n",
    "\n",
    "# degree = 2\n",
    "# skewness_limit = 1\n",
    "# num_of_cooks = 2\n",
    "# num_of_f_test = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# modeling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Linear Regression Model - sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_macro = pd.read_csv('../code/data/train_macro3.csv', index_col=0)\n",
    "df_test_macro = pd.read_csv('../code/data/test_macro3.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usdrub</th>\n",
       "      <th>full_sq</th>\n",
       "      <th>life_sq</th>\n",
       "      <th>floor</th>\n",
       "      <th>num_room</th>\n",
       "      <th>kitch_sq</th>\n",
       "      <th>state</th>\n",
       "      <th>area_m</th>\n",
       "      <th>preschool_education_centers_raion</th>\n",
       "      <th>school_education_centers_raion</th>\n",
       "      <th>...</th>\n",
       "      <th>market_count_5000</th>\n",
       "      <th>room_size</th>\n",
       "      <th>avg_price_ID_metro</th>\n",
       "      <th>avg_price_ID_railroad_station_walk</th>\n",
       "      <th>avg_price_ID_big_road1</th>\n",
       "      <th>avg_price_ID_big_road2</th>\n",
       "      <th>avg_price_ID_railroad_terminal</th>\n",
       "      <th>avg_price_ID_bus_terminal</th>\n",
       "      <th>avg_price_sub_area</th>\n",
       "      <th>price_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>28.8082</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.081628e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.608626e+06</td>\n",
       "      <td>4.516778e+06</td>\n",
       "      <td>6.311649e+06</td>\n",
       "      <td>5.626025e+06</td>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>395685.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>69.4666</td>\n",
       "      <td>729.0</td>\n",
       "      <td>802.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>21.356604</td>\n",
       "      <td>123.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.060718e+08</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>224.940665</td>\n",
       "      <td>56220124.0</td>\n",
       "      <td>1.984228e+07</td>\n",
       "      <td>1.550000e+07</td>\n",
       "      <td>1.322269e+07</td>\n",
       "      <td>1.185172e+07</td>\n",
       "      <td>1.004654e+07</td>\n",
       "      <td>2.011657e+07</td>\n",
       "      <td>95122496.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      usdrub  full_sq  life_sq  floor   num_room  kitch_sq  state  \\\n",
       "min  28.8082     10.0      0.0    0.0   0.000000       0.0    1.0   \n",
       "max  69.4666    729.0    802.0   44.0  21.356604     123.0    4.0   \n",
       "\n",
       "           area_m  preschool_education_centers_raion  \\\n",
       "min  2.081628e+06                                0.0   \n",
       "max  2.060718e+08                               13.0   \n",
       "\n",
       "     school_education_centers_raion  ...  market_count_5000   room_size  \\\n",
       "min                             0.0  ...                0.0    0.000000   \n",
       "max                            14.0  ...               21.0  224.940665   \n",
       "\n",
       "     avg_price_ID_metro  avg_price_ID_railroad_station_walk  \\\n",
       "min           3000000.0                        0.000000e+00   \n",
       "max          56220124.0                        1.984228e+07   \n",
       "\n",
       "     avg_price_ID_big_road1  avg_price_ID_big_road2  \\\n",
       "min            3.608626e+06            4.516778e+06   \n",
       "max            1.550000e+07            1.322269e+07   \n",
       "\n",
       "     avg_price_ID_railroad_terminal  avg_price_ID_bus_terminal  \\\n",
       "min                    6.311649e+06               5.626025e+06   \n",
       "max                    1.185172e+07               1.004654e+07   \n",
       "\n",
       "     avg_price_sub_area   price_doc  \n",
       "min        1.000000e+06    395685.0  \n",
       "max        2.011657e+07  95122496.0  \n",
       "\n",
       "[2 rows x 143 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_macro.describe().iloc[[3, 7], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_ivs = df_train_macro._get_numeric_data().columns.drop('price_doc').tolist()\n",
    "numeric_ivs = df_train_macro._get_numeric_data().columns.tolist()\n",
    "categorial_ivs = list(df_train_macro.dtypes[df_train_macro.dtypes == object].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_dummies = pd.get_dummies(df_train_macro[categorial_ivs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real = df_train_macro[numeric_ivs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_macro = pd.concat([df_real, df_cat_dummies], axis=1)\n",
    "cols = list(df_train_macro.columns.values)\n",
    "cols.pop(cols.index('price_doc'))\n",
    "df_train_macro = df_train_macro[cols + ['price_doc']]\n",
    "df_train_macro.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfy = df_train_macro.iloc[:, -1]\n",
    "dfx = df_train_macro.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dfx, dfy, test_size=0.3, random_state=1)\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(X_train)\n",
    "X_train_std_scaler = std_scaler.transform(X_train)\n",
    "std_scaler.fit(X_test)\n",
    "X_test_std_scaler = std_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "robust_scaler.fit(X_train)\n",
    "X_train_robust_scaler = robust_scaler.transform(X_train)\n",
    "robust_scaler.fit(X_test)\n",
    "X_train_robust_scaler = robust_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sm.add_constant(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sm.OLS(y_train, X_train).fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std_scaler = sm.add_constant(X_train_std_scaler)\n",
    "result_std_scaler = sm.OLS(y_train, X_train_std_scaler).fit()\n",
    "result_std_scaler.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_robust_scaler = sm.add_constant(X_train_robust_scaler)\n",
    "# result_robust_scaler = sm.OLS(y_train, X_train_robust_scaler).fit()\n",
    "# result_robust_scaler.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = []\n",
    "for col in  list(df_train_macro.columns):\n",
    "    col = col.replace('-', '_').replace('+', '_').replace(':', '_').replace('~', '_').replace('*', '_')\n",
    "    new_cols.append('_'+col)\n",
    "df_train_macro.columns = new_cols\n",
    "\n",
    "new_cols = []\n",
    "for col in list(df_test_macro.columns):\n",
    "    col = col.replace('-', '_').replace('+', '_').replace(':', '_').replace('~', '_').replace('*', '_')\n",
    "    new_cols.append('_'+col)\n",
    "df_test_macro.columns = new_cols\n",
    "\n",
    "categorial_ivs = list(set(df_train_macro.columns) - set(df_train_macro._get_numeric_data().columns))\n",
    "numeric_ivs = df_train_macro._get_numeric_data().columns.drop('_price_doc').tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transformation\n",
    "Transform data with skewness greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_log = []\n",
    "for f in df_train_macro._get_numeric_data().columns:\n",
    "    skewness = sp.stats.skew(df_train_macro[f])\n",
    "    if skewness > skewness_limit:\n",
    "        features_to_log.append(f)\n",
    "\n",
    "for col in df_train_macro._get_numeric_data().columns:\n",
    "    if col != '_price_doc':\n",
    "        min_val_train = min(df_train_macro[col])\n",
    "        min_val_test  = min(df_test_macro[col])\n",
    "        min_val = min(min_val_train, min_val_test)\n",
    "        if min_val <= 0:\n",
    "            df_train_macro[col] += (np.abs(min_val) + 0.1)\n",
    "            df_test_macro[col]  += (np.abs(min_val) + 0.1)\n",
    "    else:\n",
    "        min_val_train = min(df_train_macro[col])\n",
    "        if min_val_train <= 0:\n",
    "            df_train_macro[col] += (np.abs(min_val_train) + 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = sh.make_statsmodels_ols_formula(numeric_ivs, categorial_ivs, '_price_doc', log_vs=features_to_log, degree=degree, scale=False)\n",
    "model = sm.OLS.from_formula(formula, data=df_train_macro)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition Number\n",
    "Large condition number occurs when the scale of data changes significantly due to the unit difference. Scaling can decrease condition number. Multicollinearity can also cause large condition number. We can handle this by reducing dimensionality with variance inflation factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaling\n",
    "Standalize variables by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = sh.make_statsmodels_ols_formula(numeric_ivs, categorial_ivs, '_price_doc', log_vs=features_to_log, degree=degree, scale=True)\n",
    "model = sm.OLS.from_formula(formula, data=df_train_macro)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling did not significantly decrease the condition number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dimensionality Reduction\n",
    "## ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova = sm.stats.anova_lm(result, typ=2)\n",
    "anova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can remove features with p-value equal or greater than 0.05 since they have very small influences on the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-test and Feature Influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, sms_vars, formula = fs.by_f_test(df_train_macro, formula, repeat=num_of_f_test)\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Outlier\n",
    "## Cook's Distance\n",
    "- Find data with large leverage and residual by calculating Cook's distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_macro_with_outliers = df_train_macro.copy(deep=True)\n",
    "df_train_macro, model, result = pp.remove_outliers(df_train_macro, formula, repeat=3)\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regularization\n",
    "## Lasso\n",
    "Find variables with zero coefficient when Lasso regularization is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lasso = model.fit_regularized(alpha=0.001, L1_wt=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove features with zero coefficient to reduce dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_vars = []\n",
    "for idx, coef in enumerate(result_lasso.params):\n",
    "    if coef ==0:\n",
    "        continue\n",
    "    feature = result_lasso.params.index[idx]\n",
    "    if feature == 'Intercept':\n",
    "        continue\n",
    "    startDelPos = feature.find('[')\n",
    "    endDelPos = feature.find(']')\n",
    "    feature = feature.replace(feature[startDelPos:endDelPos+1], '')\n",
    "    sms_vars.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'np.log(_price_doc) ~ ' + \" + \".join(sms_vars)\n",
    "model =sm.OLS.from_formula(formula, data=df_train_macro)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Diagnosis of Regression\n",
    "## Residual Normality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier remove result \n",
    "sp.stats.probplot(result.resid, plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sms.omni_normtest(result.resid)\n",
    "for xi in zip(['Chi^2', 'P-value'], test):\n",
    "    print(\"%-12s: %6.3f\" % xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Regression Plot\n",
    "Let's visualize the influence of a single independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,70))\n",
    "sm.graphics.plot_partregress_grid(result, fig=fig)\n",
    "fig.suptitle(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = dmatrix(\" + \".join(sms_vars) + ' + np.log(_price_doc)', df_train_macro_with_outliers, return_type=\"dataframe\")\n",
    "X = dm[dm.columns.drop(['np.log(_price_doc)'])]\n",
    "y = dm['np.log(_price_doc)']\n",
    "cv = cv = KFold(n_splits=1000, shuffle=True, random_state=0)\n",
    "r2s = cross_val_score(SMWrapper(sm.OLS), X, y, scoring='r2', cv=cv)\n",
    "r2s.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(r2s, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.exp(result.predict(df_test_macro))\n",
    "y_pred = y_pred.to_frame('price_doc')\n",
    "y_pred.to_csv('./data/stats_models_{}.csv'.format(datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')), header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.39773"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
