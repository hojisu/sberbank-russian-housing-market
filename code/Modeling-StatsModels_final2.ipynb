{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with StatsModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ordinary Least Square\n",
    "- Column Names\n",
    "- Log Transformation\n",
    "- Condition Number\n",
    "- Standard Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dimensionality Reduction\n",
    "- ANOVA\n",
    "- F-test and Feature Influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Outlier\n",
    "- Cook's Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regularization\n",
    "- Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Diagnosis of Regression\n",
    "- Residual Normality Test\n",
    "- Partial Regression Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cross Validatoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Test\n",
    "- score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import warnings\n",
    "import sys\n",
    "import datetime\n",
    "import scipy as sp\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrix\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import utils.statsmodel_helper as sh\n",
    "import utils.feature_selection as fs\n",
    "import utils.preprocessing as pp\n",
    "import utils.error_calculator as ec\n",
    "\n",
    "class SMWrapper(BaseEstimator, RegressorMixin):\n",
    "    \"\"\" A universal sklearn-style wrapper for statsmodels regressors \"\"\"\n",
    "    def __init__(self, model_class, fit_intercept=True):\n",
    "        self.model_class = model_class\n",
    "        self.fit_intercept = fit_intercept\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = sm.add_constant(X)\n",
    "        self.model_ = self.model_class(y, X)\n",
    "        self.results_ = self.model_.fit()\n",
    "    def predict(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = sm.add_constant(X)\n",
    "        return self.results_.predict(X)\n",
    "\n",
    "df_train_macro = pd.read_csv('../code/data/train_macro3.csv', index_col=0)\n",
    "df_test_macro = pd.read_csv('../code/data/test_macro3.csv', index_col=0)\n",
    "\n",
    "sys.setrecursionlimit(1500)\n",
    "\n",
    "degree = 2\n",
    "skewness_limit = 1\n",
    "num_of_cooks = 2\n",
    "num_of_f_test = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_list(df,length):\n",
    "    \n",
    "    #int, float type\n",
    "    int_count = df[list(df.dtypes[df.dtypes == int].index)].apply(lambda column : len(column.unique()), axis = 0)\n",
    "    float_count = df[list(df.dtypes[df.dtypes == float].index)].apply(lambda column : len(column.unique()), axis = 0)\n",
    "    \n",
    "    int_cat_column = list(int_count[int_count < length].index)\n",
    "    float_cat_column = list(float_count[float_count < length].index)\n",
    "    \n",
    "    return int_cat_column + float_cat_column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in category_list(df_train_macro, 10):\n",
    "    print(col, df_train_macro[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_ivs = df_train_macro._get_numeric_data().columns.drop('price_doc').tolist()\n",
    "categorial_ivs = list(df_train_macro.dtypes[df_train_macro.dtypes == object].index)\n",
    "categorial_ivs.append('state')\n",
    "categorial_ivs.append('material')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_ivs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Column Names\n",
    "## Column Names\n",
    "Replace -, +, :, ~, * in column name with underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = []\n",
    "for col in  list(df_train_macro.columns):\n",
    "    col = col.replace('-', '_').replace('+', '_').replace(':', '_').replace('~', '_').replace('*', '_')\n",
    "    new_cols.append('_'+col)\n",
    "df_train_macro.columns = new_cols\n",
    "\n",
    "new_cols = []\n",
    "for col in list(df_test_macro.columns):\n",
    "    col = col.replace('-', '_').replace('+', '_').replace(':', '_').replace('~', '_').replace('*', '_')\n",
    "    new_cols.append('_'+col)\n",
    "df_test_macro.columns = new_cols\n",
    "\n",
    "categorial_ivs = list(set(df_train_macro.columns) - set(df_train_macro._get_numeric_data().columns))\n",
    "numeric_ivs = df_train_macro._get_numeric_data().columns.drop('_price_doc').tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transformation\n",
    "Transform data with skewness greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_log = []\n",
    "for f in df_train_macro._get_numeric_data().columns:\n",
    "    skewness = sp.stats.skew(df_train_macro[f])\n",
    "    if skewness > skewness_limit:\n",
    "        features_to_log.append(f)\n",
    "\n",
    "for col in df_train_macro._get_numeric_data().columns:\n",
    "    if col != '_price_doc':\n",
    "        min_val_train = min(df_train_macro[col])\n",
    "        min_val_test  = min(df_test_macro[col])\n",
    "        min_val = min(min_val_train, min_val_test)\n",
    "        if min_val <= 0:\n",
    "            df_train_macro[col] += (np.abs(min_val) + 0.1)\n",
    "            df_test_macro[col]  += (np.abs(min_val) + 0.1)\n",
    "    else:\n",
    "        min_val_train = min(df_train_macro[col])\n",
    "        if min_val_train <= 0:\n",
    "            df_train_macro[col] += (np.abs(min_val_train) + 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = sh.make_statsmodels_ols_formula(numeric_ivs, categorial_ivs, '_price_doc', log_vs=features_to_log, degree=degree, scale=False)\n",
    "model = sm.OLS.from_formula(formula, data=df_train_macro)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition Number\n",
    "Large condition number occurs when the scale of data changes significantly due to the unit difference. Scaling can decrease condition number. Multicollinearity can also cause large condition number. We can handle this by reducing dimensionality with variance inflation factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaling\n",
    "Standalize variables by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = sh.make_statsmodels_ols_formula(numeric_ivs, categorial_ivs, '_price_doc', log_vs=features_to_log, degree=degree, scale=True)\n",
    "model = sm.OLS.from_formula(formula, data=df_train_macro)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling did not significantly decrease the condition number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dimensionality Reduction\n",
    "## ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova = sm.stats.anova_lm(result, typ=2)\n",
    "anova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can remove features with p-value equal or greater than 0.05 since they have very small influences on the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-test and Feature Influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, sms_vars, formula = fs.by_f_test(df_train_macro, formula, repeat=num_of_f_test)\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Outlier\n",
    "## Cook's Distance\n",
    "- Find data with large leverage and residual by calculating Cook's distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_macro_with_outliers = df_train_macro.copy(deep=True)\n",
    "df_train_macro, model, result = pp.remove_outliers(df_train_macro, formula, repeat=3)\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regularization\n",
    "## Lasso\n",
    "Find variables with zero coefficient when Lasso regularization is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lasso = model.fit_regularized(alpha=0.001, L1_wt=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove features with zero coefficient to reduce dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_vars = []\n",
    "for idx, coef in enumerate(result_lasso.params):\n",
    "    if coef ==0:\n",
    "        continue\n",
    "    feature = result_lasso.params.index[idx]\n",
    "    if feature == 'Intercept':\n",
    "        continue\n",
    "    startDelPos = feature.find('[')\n",
    "    endDelPos = feature.find(']')\n",
    "    feature = feature.replace(feature[startDelPos:endDelPos+1], '')\n",
    "    sms_vars.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'np.log(_price_doc) ~ ' + \" + \".join(sms_vars)\n",
    "model =sm.OLS.from_formula(formula, data=df_train_macro)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Diagnosis of Regression\n",
    "## Residual Normality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier remove result \n",
    "sp.stats.probplot(result.resid, plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sms.omni_normtest(result.resid)\n",
    "for xi in zip(['Chi^2', 'P-value'], test):\n",
    "    print(\"%-12s: %6.3f\" % xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Regression Plot\n",
    "Let's visualize the influence of a single independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,70))\n",
    "sm.graphics.plot_partregress_grid(result, fig=fig)\n",
    "fig.suptitle(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = dmatrix(\" + \".join(sms_vars) + ' + np.log(_price_doc)', df_train_macro_with_outliers, return_type=\"dataframe\")\n",
    "X = dm[dm.columns.drop(['np.log(_price_doc)'])]\n",
    "y = dm['np.log(_price_doc)']\n",
    "cv = cv = KFold(n_splits=1000, shuffle=True, random_state=0)\n",
    "r2s = cross_val_score(SMWrapper(sm.OLS), X, y, scoring='r2', cv=cv)\n",
    "r2s.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(r2s, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.exp(result.predict(df_test_macro))\n",
    "y_pred = y_pred.to_frame('price_doc')\n",
    "y_pred.to_csv('./data/stats_models_{}.csv'.format(datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')), header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.39773"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
