{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Scikit-Learn Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "- Lable-Encoding\n",
    "- Log nomalization\n",
    "- Standard scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regressors : Scikit-Learn\n",
    "- DecisionTree Regressor\n",
    "- RandomForest Regressor\n",
    "- Support Vector Regressor\n",
    "- XGB Regressor\n",
    "- Cross-validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Score by Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('github'))))\n",
    "import utils.statsmodel_helper as sh\n",
    "import utils.feature_selection as fs\n",
    "import utils.preprocessing as pp\n",
    "import utils.error_calculator as ec\n",
    "import utils.helpermodeling as hm\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.model_selection import KFold, ParameterGrid, cross_val_score, cross_val_predict, GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.metrics import r2_score, explained_variance_score\n",
    "\n",
    "# model import\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(y_test, pred, measured, preds):\n",
    "    print('\\n')\n",
    "    print(\"Train Test Split\")\n",
    "    print('RMSE:', np.sqrt(mean_squared_error(y_test, pred)))\n",
    "\n",
    "    print('\\n')\n",
    "    print(\"Cross Validation\")\n",
    "    print('RMSE:', np.sqrt(mean_squared_error(measured, preds)))\n",
    "\n",
    "    fig  = plt.figure(figsize=(8, 4), dpi=100)\n",
    "    axes1 = fig.add_subplot(121)\n",
    "    axes1.scatter(y_test, pred, c='red', s=5)\n",
    "    axes1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "    axes1.set_title(\"Train Test Split\")\n",
    "    axes1.set_xlabel('Measured')\n",
    "    axes1.set_ylabel('Predicted')\n",
    "    axes2 = fig.add_subplot(122)\n",
    "    axes2.scatter(measured, preds, c='red', s=5)\n",
    "    axes2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "    axes2.set_title(\"Cross Validation\")\n",
    "    axes2.set_xlabel('Measured')\n",
    "    axes2.set_ylabel('Predicted')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    fig  = plt.figure(figsize=(8, 4), dpi=100);\n",
    "    axes1 = fig.add_subplot(121); \n",
    "    axes1.scatter(y_test, y_test-pred, c='red', s=2)\n",
    "    axes1.set_title(\"Train Test Split\")\n",
    "    axes1.set_xlabel('Measured')\n",
    "    axes1.set_ylabel('Residual')\n",
    "    axes2 = fig.add_subplot(122); \n",
    "    axes2.scatter(measured, measured-preds, c='red', s=2)\n",
    "    axes2.set_title(\"Cross Validation\")\n",
    "    axes2.set_xlabel('Measured')\n",
    "    axes2.set_ylabel('Residual')\n",
    "    fig.tight_layout();\n",
    "    \n",
    "    fig  = plt.figure(figsize=(8, 4), dpi=100)\n",
    "    axes1 = fig.add_subplot(121)\n",
    "    axes2 = fig.add_subplot(122)\n",
    "    sns.distplot((y_test-pred), bins=50, ax=axes1, axlabel='Error Deviation', kde_kws={\"color\": \"k\", \"lw\": 1.5, \"gridsize\":1000}, hist_kws={\"linewidth\": 3, \"alpha\": 0.6, \"color\": \"red\"})\n",
    "    sns.distplot((measured-preds), bins=50, ax=axes2, axlabel='Error Deviation', kde_kws={\"color\": \"k\", \"lw\": 1.5, \"gridsize\":1000}, hist_kws={\"linewidth\": 3, \"alpha\": 0.6, \"color\": \"red\"})\n",
    "    axes1.set_title(\"Train Test Split\")\n",
    "    axes2.set_title(\"Cross Validation\")\n",
    "    axes1.set_xlim(-3, 3)\n",
    "    axes2.set_xlim(-3, 3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/train_macro_without_outliers.csv', index_col=0, parse_dates=['timestamp'])\n",
    "df_train_augmented = pd.read_csv('../input/train_macro_with_outliers.csv', index_col=0, parse_dates=['timestamp'])\n",
    "df_test = pd.read_csv('../input/test_macro.csv', index_col=0, parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24976, 244) (30404, 244) (7662, 243)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape, df_train_augmented.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_features = [\n",
    " 'detention_facility_raion',\n",
    " 'culture_objects_top_25',\n",
    " 'railroad_1line',\n",
    " 'big_market_raion',\n",
    " 'incineration_raion',\n",
    " 'big_road1_1line',\n",
    " 'radiation_raion',\n",
    " 'railroad_terminal_raion',\n",
    " 'ecology',\n",
    " 'thermal_power_plant_raion',\n",
    " 'oil_chemistry_raion',\n",
    " 'nuclear_reactor_raion',\n",
    " 'product_type',\n",
    " 'water_1line'\n",
    "]\n",
    "\n",
    "numeric_features = list(df_train.columns.drop(cate_features + ['price_doc'] + ['timestamp']).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lable-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in cate_features:\n",
    "    df_train[f].fillna(df_train[f].value_counts().index[0], inplace=True)\n",
    "    df_test[f].fillna(df_test[f].value_counts().index[0], inplace=True)\n",
    "    df_train_augmented[f].fillna(df_train_augmented[f].value_counts().index[0], inplace=True)\n",
    "    \n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(df_train[f].values)\n",
    "    df_train[f] = lbl.transform(df_train[f].values)\n",
    "    df_test[f] = lbl.transform(df_test[f].values)\n",
    "    df_train_augmented[f] = LabelEncoder().fit_transform(df_train_augmented[f].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log nomalizaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Normalization of Numeric Features\n",
    "for column in numeric_features + ['price_doc']:\n",
    "    if stats.skew(df_train[column].values) > 1:\n",
    "        df_train[column] = np.log(df_train[column] + 1)  \n",
    "        df_train_augmented[column] = np.log(df_train_augmented[column] + 1)\n",
    "        if column in df_test.columns.values:\n",
    "            df_test[column]  = np.log(df_test[column] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 0 표준편차 1이 되도록 스케일링\n",
    "train_scaler = StandardScaler()\n",
    "train_scaler.fit(df_train[numeric_features])\n",
    "\n",
    "scaled_numeric_train_X = train_scaler.transform(df_train[numeric_features])\n",
    "df_scaled_numeric_train_X = pd.DataFrame(scaled_numeric_train_X, index=df_train.index, columns=numeric_features)\n",
    "df_train = pd.concat([df_scaled_numeric_train_X, df_train[cate_features], df_train['price_doc']], axis=1)\n",
    "\n",
    "scaled_numeric_test_X = train_scaler.transform(df_test[numeric_features])\n",
    "df_scaled_numeric_test_X = pd.DataFrame(scaled_numeric_test_X, index=df_test.index, columns=numeric_features)\n",
    "df_test = pd.concat([df_scaled_numeric_test_X, df_test[cate_features]], axis=1)\n",
    "\n",
    "train_scaler = StandardScaler()\n",
    "train_scaler.fit(df_train_augmented[numeric_features])\n",
    "\n",
    "scaled_numeric_train_X = train_scaler.transform(df_train_augmented[numeric_features])\n",
    "df_scaled_numeric_train_X = pd.DataFrame(scaled_numeric_train_X, index=df_train_augmented.index, columns=numeric_features)\n",
    "df_train_augmented = pd.concat([df_scaled_numeric_train_X, df_train_augmented[cate_features], df_train_augmented['price_doc']],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regressors : Scikit-Learn\n",
    "- Decision Tree Regressor\n",
    "- RandomForest Regressor\n",
    "- Support Vector Regressor\n",
    "- XGB(Extreme Gradient Boosting) Regressor\n",
    "- Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train[numeric_features+cate_features], df_train['price_doc'], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________\n",
      "Decision Tree Regressor\n",
      "______________________________________________\n",
      "R2 score. Train:  0.9999983775091627\n",
      "R2 score. Test:  0.8903502047296192\n",
      "______________________________________________\n",
      "MSE score. Train:  2.571249335533897e-07\n",
      "MSE score. Test:  0.018341308386849765\n",
      "______________________________________________\n",
      "RMSE score. Train:  0.0005070748796315882\n",
      "RMSE score. Test:  0.0005070748796315882\n",
      "______________________________________________\n",
      "MAE score. Train:  1.2031462530498222e-05\n",
      "MAE score. Test:  0.09445622911880402\n"
     ]
    }
   ],
   "source": [
    "dtr = DecisionTreeRegressor()\n",
    "y_train_dtr, y_test_dtr = hm.regression(dtr, X_train, X_test, y_train)\n",
    "hm.scores('Decision Tree Regressor', y_train, y_test, y_train_dtr, y_test_dtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________\n",
      "RandomForest Regressor\n",
      "______________________________________________\n",
      "R2 score. Train:  0.988934265522967\n",
      "R2 score. Test:  0.9418754098007451\n",
      "______________________________________________\n",
      "MSE score. Train:  0.0017536470324348084\n",
      "MSE score. Test:  0.009722599399980606\n",
      "______________________________________________\n",
      "RMSE score. Train:  0.041876569014603\n",
      "RMSE score. Test:  0.041876569014603\n",
      "______________________________________________\n",
      "MAE score. Train:  0.028201681744110238\n",
      "MAE score. Test:  0.07004242968120956\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor()\n",
    "y_train_rfr, y_test_rfr = hm.regression(rfr, X_train, X_test, y_train)\n",
    "hm.scores('RandomForest Regressor', y_train, y_test, y_train_rfr, y_test_rfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________\n",
      "Support Vector Regressor\n",
      "______________________________________________\n",
      "R2 score. Train:  0.9357326739411266\n",
      "R2 score. Test:  0.9314934784178499\n",
      "______________________________________________\n",
      "MSE score. Train:  0.010184792149095805\n",
      "MSE score. Test:  0.011459202780545517\n",
      "______________________________________________\n",
      "RMSE score. Train:  0.100919731217913\n",
      "RMSE score. Test:  0.100919731217913\n",
      "______________________________________________\n",
      "MAE score. Train:  0.08159051016507356\n",
      "MAE score. Test:  0.084696464226569\n"
     ]
    }
   ],
   "source": [
    "svr = SVR(gamma='scale', C=1.0, epsilon=0.2)\n",
    "y_train_svr, y_test_svr = hm.regression(svr, X_train, X_test, y_train)\n",
    "hm.scores('Support Vector Regressor', y_train, y_test, y_train_svr, y_test_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=10, min_child_weight=1, missing=None, n_estimators=200,\n",
      "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n"
     ]
    }
   ],
   "source": [
    "param_grid = [{\n",
    "    'max_depth': [10],\n",
    "    'learning_rate' : [0.1],\n",
    "    'n_estimators' : [200], \n",
    "    'colsample_bytree': [0.5]\n",
    "}]\n",
    "\n",
    "xgr = xgb.XGBRegressor()\n",
    "grid_xgr = GridSearchCV(xgr, param_grid, cv=cv, n_jobs=4, scoring='neg_mean_squared_log_error')\n",
    "grid_xgr.fit(X_train, y_train)\n",
    "print(grid_xgr.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgr = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=10, min_child_weight=1, missing=None, n_estimators=200,\n",
    "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-473b5edb240e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train_xgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_xgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'XGB Regressor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_xgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_xgr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/Russian_Housing_project/github/utils/helpermodeling.py\u001b[0m in \u001b[0;36mregression\u001b[0;34m(regressor, x_train, x_test, y_train)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0my_train_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    371\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1045\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_train_xgr, y_test_xgr = hm.regression(xgr, X_train, X_test, y_train)\n",
    "hm.scores('XGB Regressor', y_train, y_test, y_train_xgr, y_test_xgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = grid_xgr.best_estimator_.predict(X_test)\n",
    "sns.distplot(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(grid_xgr.best_estimator_, \n",
    "                                df_train[numeric_features+cate_features], \n",
    "                                df_train['price_doc'], \n",
    "                                cv=cv)\n",
    "\n",
    "show_results(y_test, \n",
    "             prediction, \n",
    "             df_train['price_doc'], \n",
    "             predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = grid_xgr.best_estimator_.predict(df_test)\n",
    "sns.distplot(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_r2 = cross_val_score(dtr, X_train, y_train, scoring='r2', cv=cv)\n",
    "rfr_r2 = cross_val_score(rfr, X_train, y_train, scoring='r2', cv=cv)\n",
    "svr_r2 = cross_val_score(svr, X_train, y_train, scoring='r2', cv=cv)\n",
    "xgr_r2 = cross_val_score(xgr, X_train, y_train, scoring='r2', cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(actual_values, predicted_values, convertExp=True):\n",
    "    \"\"\"\n",
    "    - root mean squared log error는 error를 로그화값으로 변환하고, 제곱하고, 평균을 내고, 루트를 씌웁니다.\n",
    "    - skewness를 해결하기 위해 np.log1p를 했기 때문에, 값을 예측할 때 이를 다시 변환해서 처리해주는 것이 필요합니다. \n",
    "    \"\"\"\n",
    "    if convertExp==True:\n",
    "        predicted_values = np.exp(predicted_values),\n",
    "        actual_values = np.array(np.exp(actual_values))\n",
    "        \n",
    "    log_predicted_values = np.log(np.array(predicted_values)+1)\n",
    "    log_actual_values = np.log(np.array(actual_values)+1)\n",
    "\n",
    "    # 위에서 계산한 예측값에서 실제값을 빼주고 제곱을 해준다.\n",
    "    difference = np.square(log_predicted_values - log_actual_values)\n",
    "    return np.sqrt(difference.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_train_rmsle = - cross_val_score(dtr, X_train, y_train, scoring=make_scorer(rmsle, greater_is_better=False), cv=cv)\n",
    "rfr_train_rmsle = - cross_val_score(rfr, X_train, y_train, scoring=make_scorer(rmsle, greater_is_better=False), cv=cv)\n",
    "svr_train_rmsle = - cross_val_score(svr, X_train, y_train, scoring=make_scorer(rmsle, greater_is_better=False), cv=cv)\n",
    "xgr_train_rmsle = - cross_val_score(xgr, X_train, y_train, scoring=make_scorer(rmsle, greater_is_better=False), cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_test_rmsle = - cross_val_score(dtr, X_test, y_test, scoring=make_scorer(rmsle, greater_is_better=False), cv=cv)\n",
    "rfr_test_rmsle = - cross_val_score(rfr, X_test, y_test, scoring=make_scorer(rmsle, greater_is_better=False), cv=cv)\n",
    "svr_test_rmsle = - cross_val_score(svr, X_test, y_test, scoring=make_scorer(rmsle, greater_is_better=False), cv=cv)\n",
    "xgr_test_rmsle = - cross_val_score(xgr, X_test, y_test, scoring=make_scorer(rmsle, greater_is_better=False), cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score = pd.DataFrame({'DecisionTreeRegressor' : dtr_r2, 'RandomForestRegressor' : rfr_r2, 'SupportVectorRegressor' : svr_r2, 'XGBRegressor' : xgr_r2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle_train = pd.DataFrame({'DecisionTreeRegressor' : dtr_train_rmsle, 'RandomForestRegressor' : rfr_train_rmsle, 'SupportVectorRegressor' : svr_train_rmsle, 'XGBRegressor' : xgr_train_rmsle})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle_test = pd.DataFrame({'DecisionTreeRegressor' : dtr_test_rmsle, 'RandomForestRegressor' : rfr_test_rmsle, 'SupportVectorRegressor' : svr_test_rmsle, 'XGBRegressor' : xgr_test_rmsle})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## boxplot\n",
    "plt.figure(figsize=(12, 7))\n",
    "r2_score.boxplot(column= ['DecisionTreeRegressor', 'RandomForestRegressor', 'SupportVectorRegressor', 'XGBRegressor']) \n",
    "plt.xticks(size = 10, rotation=45)\n",
    "plt.yticks(size = 10)\n",
    "plt.title('r2 by Model')\n",
    "plt.ylabel(\"r2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## boxplot\n",
    "plt.figure(figsize=(12, 7))\n",
    "rmsle_train.boxplot(column= ['DecisionTreeRegressor', 'RandomForestRegressor', 'SupportVectorRegressor', 'XGBRegressor']) \n",
    "plt.xticks(size = 10, rotation=45)\n",
    "plt.yticks(size = 10)\n",
    "plt.title('RMSLE_train by Model')\n",
    "plt.ylabel(\"RMSLE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## boxplot\n",
    "plt.figure(figsize=(12, 7))\n",
    "rmsle_test.boxplot(column= ['DecisionTreeRegressor', 'RandomForestRegressor', 'SupportVectorRegressor', 'XGBRegressor']) \n",
    "plt.xticks(size = 10, rotation=45)\n",
    "plt.yticks(size = 10)\n",
    "plt.title('RMSLE_test by Model')\n",
    "plt.ylabel(\"RMSLE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
