{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature/Data Transformation\n",
    "- Outliers\n",
    "- New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Data Imputation\n",
    "- regression\n",
    "- Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dimensionality Reduction\n",
    "- Features with Bad or Constant Data\n",
    "- Multicollinearity and Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import scipy as sp\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrix\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('github'))))\n",
    "import utils.preprocessing as pp \n",
    "import utils.correlation as cr\n",
    "import utils.statsmodel_helper as st\n",
    "import utils.var_inflation_factor as vif\n",
    "\n",
    "df_macro = pd.read_csv('../input/macro.csv', parse_dates=['timestamp'])\n",
    "df_train = pd.read_csv('../input/train.csv', index_col=0, parse_dates=['timestamp'])\n",
    "df_test = pd.read_csv('../input/test.csv', index_col=0, parse_dates=['timestamp'])\n",
    "\n",
    "min_corr = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature/Data Transformation\n",
    "## outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop data with extremely big price #\n",
    "df_train = df_train.drop([2121]) \n",
    "\n",
    "# Replace outliers with proper value #\n",
    "df_train.loc[df_train.state == 33, 'state'] = 3\n",
    "df_train.loc[df_train['life_sq'] > 1000,     'life_sq']       = np.mean(df_train['life_sq'].dropna())\n",
    "df_train.loc[df_train['kitch_sq'] > 250,     'kitch_sq']      = np.mean(df_train['kitch_sq'].dropna())\n",
    "df_train.loc[df_train['num_room'] > 6,       'num_room']      = np.mean(df_train['num_room'].dropna())\n",
    "df_train.loc[df_train['build_year'] > 2017,  'build_year']    = np.mean(df_train['build_year'].dropna())\n",
    "df_train.loc[df_train['build_year'] < 1800,  'build_year']    = np.mean(df_train['build_year'].dropna())\n",
    "df_train.loc[df_train['floor'] > 50,         'floor']         = np.mean(df_train['floor'].dropna())\n",
    "df_train.loc[df_train['max_floor'] > 60,     'max_floor']     = np.mean(df_train['max_floor'].dropna())\n",
    "df_train.loc[df_train.full_sq == 0, 'full_sq'] = 50\n",
    "df_train = df_train[df_train.price_doc/df_train.full_sq <= 600000]\n",
    "df_train = df_train[df_train.price_doc/df_train.full_sq >= 10000]\n",
    "\n",
    "df_test.loc[df_test['life_sq'] > 1000,     'life_sq']       = np.mean(df_test['life_sq'].dropna())\n",
    "df_test.loc[df_test['kitch_sq'] > 250,     'kitch_sq']      = np.mean(df_test['kitch_sq'].dropna())\n",
    "df_test.loc[df_test['num_room'] > 6,       'num_room']      = np.mean(df_test['num_room'].dropna())\n",
    "df_test.loc[df_test['build_year'] > 2017,  'build_year']    = np.mean(df_test['build_year'].dropna())\n",
    "df_test.loc[df_test['build_year'] < 1800,  'build_year']    = np.mean(df_test['build_year'].dropna())\n",
    "df_test.loc[df_test['floor'] > 50,         'floor']         = np.mean(df_test['floor'].dropna())\n",
    "df_test.loc[df_test['max_floor'] > 60,     'max_floor']     = np.mean(df_test['max_floor'].dropna())\n",
    "df_test.loc[df_test.full_sq == 0, 'full_sq'] = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and day of week #\n",
    "df_train['month'] = df_train.timestamp.dt.month\n",
    "df_train['dow'] = df_train.timestamp.dt.dayofweek\n",
    "\n",
    "df_test['month'] = df_test.timestamp.dt.month\n",
    "df_test['dow'] = df_test.timestamp.dt.dayofweek\n",
    "\n",
    "df_train[\"yearweek\"] = df_train[\"timestamp\"].dt.year*100 + df_train[\"timestamp\"].dt.weekofyear\n",
    "df_test[\"yearweek\"] = df_test[\"timestamp\"].dt.year*100 + df_test[\"timestamp\"].dt.weekofyear\n",
    "\n",
    "# Create new features that might help #\n",
    "df_train['rel_floor'] = df_train['floor'] / df_train['max_floor'].astype(float)\n",
    "df_train['rel_kitch_sq'] = df_train['kitch_sq'] / df_train['full_sq'].astype(float)\n",
    "\n",
    "df_test['rel_floor'] = df_test['floor'] / df_test['max_floor'].astype(float)\n",
    "df_test['rel_kitch_sq'] = df_test['kitch_sq'] / df_test['full_sq'].astype(float)\n",
    "\n",
    "df_train.apartment_name=df_train.sub_area + df_train['metro_km_avto'].astype(str)\n",
    "df_test.apartment_name=df_test.sub_area + df_train['metro_km_avto'].astype(str)\n",
    "del df_train['metro_km_avto']\n",
    "del df_test['metro_km_avto']\n",
    "\n",
    "df_train['room_size'] = df_train['life_sq'] / df_train['num_room'].astype(float)\n",
    "df_test['room_size'] = df_test['life_sq'] / df_test['num_room'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average price corresponding to sub_area and ID_* #\n",
    "id_features = ['ID_metro',\n",
    "    'ID_railroad_station_walk', \\\n",
    "    'ID_big_road1', \\\n",
    "    'ID_big_road2', \\\n",
    "    'ID_railroad_terminal', \\\n",
    "    'ID_bus_terminal']\n",
    "\n",
    "for id_f in id_features:\n",
    "    df_test['avg_price_' + id_f] = 0.0\n",
    "    for val in df_test[id_f].unique():\n",
    "        if val == 171 and id_f == 'ID_metro':\n",
    "            df_test.loc[df_test.ID_metro == 171, 'avg_price_ID_metro'] = df_train[df_train.ID_metro == 170]['price_doc'].mean()\n",
    "            continue\n",
    "        if val == 132 and id_f == 'ID_railroad_station_walk':\n",
    "            df_test.loc[df_test.ID_railroad_station_walk == 132, 'avg_price_ID_railroad_station_walk'] = df_train[df_train.ID_railroad_station_walk == 131]['price_doc'].mean()\n",
    "            continue\n",
    "        if val == 121 and id_f == 'ID_railroad_station_walk':\n",
    "            df_test.loc[df_test.ID_railroad_station_walk == 122, 'avg_price_ID_railroad_station_walk'] = df_train[df_train.ID_railroad_station_walk == 131]['price_doc'].mean()\n",
    "            continue\n",
    "        avg = df_train[df_train[id_f] == val]['price_doc'].mean()\n",
    "        df_test.loc[df_test[id_f] == val, 'avg_price_' + id_f] = avg\n",
    "    del df_test[id_f]\n",
    "    \n",
    "for id_f in id_features:\n",
    "    df_train['avg_price_' + id_f] = 0.0\n",
    "    for val in df_train[id_f].unique():\n",
    "        avg = df_train[df_train[id_f] == val]['price_doc'].mean()\n",
    "        df_train.loc[df_train[id_f] == val, 'avg_price_' + id_f] = avg\n",
    "    del df_train[id_f]\n",
    "    \n",
    "cols = list(df_train.columns.values)\n",
    "cols.pop(cols.index('price_doc'))\n",
    "df_train = df_train[cols + ['price_doc']]\n",
    "\n",
    "\n",
    "df_test['avg_price_sub_area'] = 0.0\n",
    "df_train['avg_price_sub_area'] = 0.0\n",
    "for subarea in df_train['sub_area'].unique():\n",
    "    avg = df_train[df_train['sub_area'] == subarea]['price_doc'].mean()\n",
    "    df_train.loc[df_train['sub_area'] == subarea, 'avg_price_sub_area'] = avg\n",
    "    df_test.loc[df_test['sub_area'] == subarea, 'avg_price_sub_area'] = avg\n",
    "del df_train['sub_area']\n",
    "del df_test['sub_area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Data Imputation\n",
    "### regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_macro.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_train = pp.impute_by_regression(df_train, 4, 0.1)\n",
    "df_test = pp.impute_by_regression(df_test, 4, 0.1)\n",
    "df_macro = pp.impute_by_regression(df_macro, 4, 0.1)\n",
    "df_train._get_numeric_data()[df_train._get_numeric_data() < 0] = 0\n",
    "df_test._get_numeric_data()[df_test._get_numeric_data() < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_column</th>\n",
       "      <th>missing_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [missing_column, missing_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.find_missing_data_columns(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_column</th>\n",
       "      <th>missing_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>product_type</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   missing_column  missing_count\n",
       "10   product_type             33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.find_missing_data_columns(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_column</th>\n",
       "      <th>missing_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>child_on_acc_pre_school</td>\n",
       "      <td>658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>modern_education_share</td>\n",
       "      <td>1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>old_education_build_share</td>\n",
       "      <td>1389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               missing_column  missing_count\n",
       "78    child_on_acc_pre_school            658\n",
       "81     modern_education_share           1389\n",
       "82  old_education_build_share           1389"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.find_missing_data_columns(df_macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric #\n",
    "for col in df_train._get_numeric_data().columns[df_train._get_numeric_data().columns.isnull().any()].tolist():\n",
    "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
    "for col in df_test._get_numeric_data().columns[df_test._get_numeric_data().columns.isnull().any()].tolist():\n",
    "    df_test[col].fillna(df_train[col].mean(), inplace=True)\n",
    "\n",
    "# categorical #\n",
    "for col in df_train.columns[df_train.isnull().any()].tolist():\n",
    "    df_train[col].fillna(df_train[col].value_counts().index[0], inplace=True)\n",
    "for col in df_test.columns[df_test.isnull().any()].tolist():\n",
    "    df_test[col].fillna(df_train[col].value_counts().index[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dimensionality Reduction\n",
    "## Features with Bad or Constant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features with high correlation with other #\n",
    "features_to_remove = [\n",
    "    'children_preschool', 'children_school', 'male_f', \\\n",
    "    'female_f', 'young_male', 'young_female', 'work_male', \\\n",
    "    'work_female', 'ekder_male', 'ekder_female',\\\n",
    "    '0_6_all', '0_6_male', '0_6_female',\\\n",
    "    '7_14_all', '7_14_male', '7_14_female', '0_17_male', '0_17_female',\\\n",
    "    '16_29_male', '16_29_female', '0_13_all', '0_13_male', '0_13_female',\\\n",
    "]\n",
    "for f in features_to_remove:\n",
    "    del df_train[f]\n",
    "    del df_test[f]\n",
    "    \n",
    "# Macro features with bad data #\n",
    "del df_macro['modern_education_share']\n",
    "del df_macro['old_education_build_share']\n",
    "del df_macro['child_on_acc_pre_school']\n",
    "\n",
    "# Constant features #\n",
    "consts = [col for col in df_train.columns if len(df_train[col].value_counts().index) == 1]\n",
    "for const in consts:\n",
    "    del df_train[const]\n",
    "    del df_test[const]\n",
    "    \n",
    "consts = [col for col in df_macro.columns if len(df_macro[col].value_counts().index) == 1]\n",
    "for const in consts:\n",
    "    del df_macro[const]\n",
    "    \n",
    "# Low correlation with price #\n",
    "corr_limit = 0.1\n",
    "for column in df_train._get_numeric_data().columns.drop('price_doc').values:\n",
    "    if abs(df_train[column].corr(df_train['price_doc'])) < corr_limit:\n",
    "        df_train = df_train.drop(column, axis=1)\n",
    "        if column in df_test.columns.values:\n",
    "            df_test = df_test.drop(column, axis=1)\n",
    "\n",
    "corr_limit = 0.05           \n",
    "for column in df_macro._get_numeric_data().columns.values:\n",
    "    if abs(df_macro[column].corr(df_train['price_doc'])) < corr_limit:\n",
    "        df_macro = df_macro.drop(column, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity and Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[df_train==np.inf]=np.nan\n",
    "# df_train.fillna(df_train.median(), inplace=True)\n",
    "# categorial_ivs = set(df_train.columns.drop('timestamp')) - set(df_train._get_numeric_data().columns)\n",
    "# numeric_ivs = df_train._get_numeric_data().columns.drop('price_doc')\n",
    "# temp = vif.VarInflationFactor(impute=True, thresh=10.0).fit_transform(df_train[numeric_ivs])\n",
    "# df_train = pd.concat([df_train['timestamp'], temp, df_train[categorial_ivs], df_train['price_doc']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_removes = [\n",
    "    \"raion_popul\"\n",
    "    \"cafe_count_3000\" \n",
    "    \"cafe_count_5000\" \n",
    "    \"cafe_avg_price_1500\" \n",
    "    \"raion_build_count_with_builddate_info\" \n",
    "    \"kremlin_km\" \n",
    "    \"cafe_count_2000\" \n",
    "    \"sadovoe_km\" \n",
    "    \"cafe_count_1500\" \n",
    "    \"0_17_all\" \n",
    "    \"bulvar_ring_km\" \n",
    "    \"cafe_sum_1500_max_price_avg\" \n",
    "    \"cafe_count_5000_price_1000\" \n",
    "    \"yearweek\" \n",
    "    \"school_km\" \n",
    "    \"cafe_count_5000_price_1500\" \n",
    "    \"cafe_count_5000_price_2500\" \n",
    "    \"cafe_count_3000_price_1500\" \n",
    "    \"office_count_5000\" \n",
    "    \"cafe_count_1000\" \n",
    "    \"cafe_count_3000_price_500\" \n",
    "    \"office_count_3000\" \n",
    "    \"cafe_count_3000_price_2500\" \n",
    "    \"ttk_km\" \n",
    "    \"cafe_count_2000_price_1500\" \n",
    "    \"cafe_count_2000_price_500\" \n",
    "    \"cafe_count_5000_price_500\" \n",
    "    \"avg_price_ID_railroad_terminal\" \n",
    "    \"office_count_2000\" \n",
    "    \"church_count_5000\" \n",
    "    \"cafe_count_2000_price_2500\" \n",
    "    \"cafe_count_3000_price_1000\" \n",
    "    \"cafe_count_1500_price_1500\" \n",
    "    \"cafe_count_5000_na_price\" \n",
    "    \"cafe_count_2000_price_1000\" \n",
    "    \"work_all\" \n",
    "    \"zd_vokzaly_avto_km\" \n",
    "    \"church_count_3000\"\n",
    "    \"oil_chemistry_km\" \n",
    "    \"cafe_count_1500_price_500\" \n",
    "    \"cafe_count_5000_price_4000\" \n",
    "    \"avg_price_ID_bus_terminal\" \n",
    "    \"cafe_count_3000_price_4000\" \n",
    "    \"office_count_1500\"\n",
    "    \"cafe_count_3000_na_price\" \n",
    "    \"trc_count_5000\" \n",
    "    \"cafe_count_1500_price_1000\" \n",
    "    \"sport_count_5000\" \n",
    "    \"leisure_count_5000\"\n",
    "    \"avg_price_sub_area\"\n",
    "    \"radiation_km\" \n",
    "    \"big_church_count_3000\"\n",
    "    \"cafe_count_2000_na_price\"\n",
    "    \"cafe_count_1500_price_2500\"\n",
    "    \"basketball_km\" \n",
    "    \"avg_price_ID_big_road1\"\n",
    "    \"preschool_education_centers_raion\"\n",
    "    \"green_part_3000\" \n",
    "    \"cafe_count_500\" \n",
    "    \"avg_price_ID_railroad_station_walk\"\n",
    "    \"cafe_count_1000_price_1500\"\n",
    "    \"stadium_km\"\n",
    "    \"avg_price_ID_big_road2\" \n",
    "    \"leisure_count_3000\" \n",
    "    \"preschool_km\" \n",
    "    \"sport_count_3000\"\n",
    "    \"power_transmission_line_km\" \n",
    "    \"office_sqm_3000\" \n",
    "    \"museum_km\" \n",
    "    \"exhibition_km\"\n",
    "    \"cafe_count_2000_price_high\" \n",
    "    \"mosque_km\" \n",
    "    \"workplaces_km\"\n",
    "    \"cafe_count_1500_na_price\"\n",
    "    \"university_km\"\n",
    "    \"trc_count_3000\"\n",
    "    \"young_all\"\n",
    "    \"cafe_sum_2000_max_price_avg\" \n",
    "    \"office_count_1000\"\n",
    "    \"sport_count_2000\" \n",
    "    \"num_room\" \n",
    "    \"office_sqm_5000\" \n",
    "    \"thermal_power_plant_km\" \n",
    "    \"office_raion\" \n",
    "    \"swim_pool_km\" \n",
    "    \"railroad_station_avto_min\" \n",
    "    \"green_part_5000\" \n",
    "    \"trc_count_2000\" \n",
    "    \"cafe_count_1000_price_1000\"\n",
    "    \"detention_facility_km\"\n",
    "    \"big_church_km\" \n",
    "    \"cafe_count_3000_price_high\" \n",
    "    \"shopping_centers_km\"\n",
    "    \"avg_price_ID_metro\" \n",
    "    \"leisure_count_1500\" \n",
    "    \"nuclear_reactor_km\" \n",
    "    \"cafe_sum_1500_min_price_avg\" \n",
    "    \"office_sqm_2000\" \n",
    "    \"trc_sqm_5000\" \n",
    "    \"sport_objects_raion\" \n",
    "    \"park_km\" \n",
    "    \"full_sq\" \n",
    "    \"big_road2_km\"\n",
    "    \"ekder_all\" \n",
    "    \"sport_count_1500\"\n",
    "    \"state\" \n",
    "    \"public_healthcare_km\"\n",
    "    \"ts_km\" \n",
    "    \"big_church_count_5000\"\n",
    "    \"bus_terminal_avto_km\" \n",
    "    \"theater_km\" \n",
    "    \"area_m\"\n",
    "    \"room_size\" \n",
    "    \"raion_build_count_with_material_info\" \n",
    "    \"cafe_count_1500_price_high\"\n",
    "    \"office_sqm_1500\" \n",
    "    \"market_count_3000\" \n",
    "]\n",
    "\n",
    "for f in feature_to_removes:\n",
    "    if f in df_train:\n",
    "        del df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_macro = df_train.merge(df_macro, left_on='timestamp', right_on='timestamp', how='left').set_index(df_train.index)\n",
    "df_test_macro = df_test.merge(df_macro, left_on='timestamp', right_on='timestamp', how='left').set_index(df_test.index)\n",
    "cols = list(df_train_macro.columns.values)\n",
    "cols.pop(cols.index('price_doc'))\n",
    "df_train_macro = df_train_macro[cols + ['price_doc']]\n",
    "df_train_macro.to_csv('../input/train_macro.csv', header=True, index=True)\n",
    "df_test_macro.to_csv('../input/test_macro.csv', header=True, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
