{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature/Data Transformation\n",
    "- Outliers\n",
    "- New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Data Imputation\n",
    "- regression\n",
    "- Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dimensionality Reduction\n",
    "- Features with Bad or Constant Data\n",
    "- Multicollinearity and Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import scipy as sp\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrix\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('github'))))\n",
    "import utils.preprocessing as pp \n",
    "import utils.correlation as cr\n",
    "import utils.statsmodel_helper as st\n",
    "\n",
    "df_macro = pd.read_csv('../input/macro.csv', parse_dates=['timestamp'])\n",
    "df_train = pd.read_csv('../input/train.csv', index_col=0, parse_dates=['timestamp'])\n",
    "df_test = pd.read_csv('../input/test.csv', index_col=0, parse_dates=['timestamp'])\n",
    "\n",
    "min_corr = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature/Data Transformation\n",
    "## outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop data with extremely big price #\n",
    "df_train = df_train.drop([2121]) \n",
    "\n",
    "# Replace outliers with proper value #\n",
    "df_train.loc[df_train.state == 33, 'state'] = 3\n",
    "df_train.loc[df_train['life_sq'] > 1000,     'life_sq']       = np.mean(df_train['life_sq'].dropna())\n",
    "df_train.loc[df_train['kitch_sq'] > 250,     'kitch_sq']      = np.mean(df_train['kitch_sq'].dropna())\n",
    "df_train.loc[df_train['num_room'] > 6,       'num_room']      = np.mean(df_train['num_room'].dropna())\n",
    "df_train.loc[df_train['build_year'] > 2017,  'build_year']    = np.mean(df_train['build_year'].dropna())\n",
    "df_train.loc[df_train['build_year'] < 1800,  'build_year']    = np.mean(df_train['build_year'].dropna())\n",
    "df_train.loc[df_train['floor'] > 50,         'floor']         = np.mean(df_train['floor'].dropna())\n",
    "df_train.loc[df_train['max_floor'] > 60,     'max_floor']     = np.mean(df_train['max_floor'].dropna())\n",
    "df_train.loc[df_train.full_sq == 0, 'full_sq'] = 50\n",
    "df_train = df_train[df_train.price_doc/df_train.full_sq <= 600000]\n",
    "df_train = df_train[df_train.price_doc/df_train.full_sq >= 10000]\n",
    "\n",
    "df_test.loc[df_test['life_sq'] > 1000,     'life_sq']       = np.mean(df_test['life_sq'].dropna())\n",
    "df_test.loc[df_test['kitch_sq'] > 250,     'kitch_sq']      = np.mean(df_test['kitch_sq'].dropna())\n",
    "df_test.loc[df_test['num_room'] > 6,       'num_room']      = np.mean(df_test['num_room'].dropna())\n",
    "df_test.loc[df_test['build_year'] > 2017,  'build_year']    = np.mean(df_test['build_year'].dropna())\n",
    "df_test.loc[df_test['build_year'] < 1800,  'build_year']    = np.mean(df_test['build_year'].dropna())\n",
    "df_test.loc[df_test['floor'] > 50,         'floor']         = np.mean(df_test['floor'].dropna())\n",
    "df_test.loc[df_test['max_floor'] > 60,     'max_floor']     = np.mean(df_test['max_floor'].dropna())\n",
    "df_test.loc[df_test.full_sq == 0, 'full_sq'] = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and day of week #\n",
    "df_train['month'] = df_train.timestamp.dt.month\n",
    "df_train['dow'] = df_train.timestamp.dt.dayofweek\n",
    "\n",
    "df_test['month'] = df_test.timestamp.dt.month\n",
    "df_test['dow'] = df_test.timestamp.dt.dayofweek\n",
    "\n",
    "# Create new features that might help #\n",
    "df_train['rel_floor'] = df_train['floor'] / df_train['max_floor'].astype(float)\n",
    "df_train['rel_kitch_sq'] = df_train['kitch_sq'] / df_train['full_sq'].astype(float)\n",
    "\n",
    "df_test['rel_floor'] = df_test['floor'] / df_test['max_floor'].astype(float)\n",
    "df_test['rel_kitch_sq'] = df_test['kitch_sq'] / df_test['full_sq'].astype(float)\n",
    "\n",
    "df_train.apartment_name=df_train.sub_area + df_train['metro_km_avto'].astype(str)\n",
    "df_test.apartment_name=df_test.sub_area + df_train['metro_km_avto'].astype(str)\n",
    "del df_train['metro_km_avto']\n",
    "del df_test['metro_km_avto']\n",
    "\n",
    "df_train['room_size'] = df_train['life_sq'] / df_train['num_room'].astype(float)\n",
    "df_test['room_size'] = df_test['life_sq'] / df_test['num_room'].astype(float)\n",
    "\n",
    "#Average price corresponding to sub_area and ID_* #\n",
    "id_features = ['ID_metro',\n",
    "    'ID_railroad_station_walk', \\\n",
    "    'ID_big_road1', \\\n",
    "    'ID_big_road2', \\\n",
    "    'ID_railroad_terminal', \\\n",
    "    'ID_bus_terminal']\n",
    "\n",
    "for id_f in id_features:\n",
    "    df_test['avg_price_' + id_f] = 0.0\n",
    "    for val in df_test[id_f].unique():\n",
    "        if val == 171 and id_f == 'ID_metro':\n",
    "            df_test.loc[df_test.ID_metro == 171, 'avg_price_ID_metro'] = df_train[df_train.ID_metro == 170]['price_doc'].mean()\n",
    "            continue\n",
    "        if val == 132 and id_f == 'ID_railroad_station_walk':\n",
    "            df_test.loc[df_test.ID_railroad_station_walk == 132, 'avg_price_ID_railroad_station_walk'] = df_train[df_train.ID_railroad_station_walk == 131]['price_doc'].mean()\n",
    "            continue\n",
    "        if val == 121 and id_f == 'ID_railroad_station_walk':\n",
    "            df_test.loc[df_test.ID_railroad_station_walk == 122, 'avg_price_ID_railroad_station_walk'] = df_train[df_train.ID_railroad_station_walk == 131]['price_doc'].mean()\n",
    "            continue\n",
    "        avg = df_train[df_train[id_f] == val]['price_doc'].mean()\n",
    "        df_test.loc[df_test[id_f] == val, 'avg_price_' + id_f] = avg\n",
    "    del df_test[id_f]\n",
    "    \n",
    "for id_f in id_features:\n",
    "    df_train['avg_price_' + id_f] = 0.0\n",
    "    for val in df_train[id_f].unique():\n",
    "        avg = df_train[df_train[id_f] == val]['price_doc'].mean()\n",
    "        df_train.loc[df_train[id_f] == val, 'avg_price_' + id_f] = avg\n",
    "    del df_train[id_f]\n",
    "    \n",
    "cols = list(df_train.columns.values)\n",
    "cols.pop(cols.index('price_doc'))\n",
    "df_train = df_train[cols + ['price_doc']]\n",
    "\n",
    "\n",
    "df_test['avg_price_sub_area'] = 0.0\n",
    "df_train['avg_price_sub_area'] = 0.0\n",
    "for subarea in df_train['sub_area'].unique():\n",
    "    avg = df_train[df_train['sub_area'] == subarea]['price_doc'].mean()\n",
    "    df_train.loc[df_train['sub_area'] == subarea, 'avg_price_sub_area'] = avg\n",
    "    df_test.loc[df_test['sub_area'] == subarea, 'avg_price_sub_area'] = avg\n",
    "del df_train['sub_area']\n",
    "del df_test['sub_area']\n",
    "\n",
    "\n",
    "# Add the Macro Feature #\n",
    "usdrub_pairs = dict(zip(list(df_macro['timestamp']), list(df_macro['usdrub'])))\n",
    "\n",
    "df_train['timestamp'].replace(usdrub_pairs,inplace=True)\n",
    "df_test['timestamp'].replace(usdrub_pairs,inplace=True)\n",
    "\n",
    "df_train.rename(columns={'timestamp' : 'usdrub'}, inplace=True)\n",
    "df_test.rename(columns={'timestamp' : 'usdrub'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Data Imputation\n",
    "### regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_train = pp.impute_by_regression(df_train, 4, 0.1)\n",
    "df_test = pp.impute_by_regression(df_test, 4, 0.1)\n",
    "df_train._get_numeric_data()[df_train._get_numeric_data() < 0] = 0\n",
    "df_test._get_numeric_data()[df_test._get_numeric_data() < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_column</th>\n",
       "      <th>missing_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [missing_column, missing_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.find_missing_data_columns(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_column</th>\n",
       "      <th>missing_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>product_type</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   missing_column  missing_count\n",
       "10   product_type             33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.find_missing_data_columns(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric #\n",
    "for col in df_train._get_numeric_data().columns[df_train._get_numeric_data().columns.isnull().any()].tolist():\n",
    "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
    "for col in df_test._get_numeric_data().columns[df_test._get_numeric_data().columns.isnull().any()].tolist():\n",
    "    df_test[col].fillna(df_train[col].mean(), inplace=True)\n",
    "\n",
    "# categorical #\n",
    "for col in df_train.columns[df_train.isnull().any()].tolist():\n",
    "    df_train[col].fillna(df_train[col].value_counts().index[0], inplace=True)\n",
    "for col in df_test.columns[df_test.isnull().any()].tolist():\n",
    "    df_test[col].fillna(df_train[col].value_counts().index[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dimensionality Reduction\n",
    "## Features with Bad or Constant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features with high correlation with other #\n",
    "features_to_remove = [\n",
    "    'school_quota', 'preschool_quota', 'children_preschool', 'children_school', 'male_f', \\\n",
    "    'female_f', 'young_male', 'young_female', 'work_male', \\\n",
    "    'work_female', 'ekder_male', 'ekder_female',\\\n",
    "    '0_6_all', '0_6_male', '0_6_female',\\\n",
    "    '7_14_all', '7_14_male', '7_14_female', '0_17_male', '0_17_female',\\\n",
    "    '16_29_male', '16_29_female', '0_13_all', '0_13_male', '0_13_female',\\\n",
    "    'metro_km_walk', 'railroad_station_walk_km',\\\n",
    "    'railroad_station_avto_km', 'public_transport_station_km', 'build_count_1971-1995',\n",
    "    'build_count_panel', 'metro_min_walk'\\\n",
    "]\n",
    "for f in features_to_remove:\n",
    "    del df_train[f]\n",
    "    del df_test[f]\n",
    "    \n",
    "# Macro features with bad data #\n",
    "del df_macro['modern_education_share']\n",
    "del df_macro['old_education_build_share']\n",
    "del df_macro['child_on_acc_pre_school']\n",
    "\n",
    "# Constant features #\n",
    "consts = [col for col in df_train.columns if len(df_train[col].value_counts().index) == 1]\n",
    "for const in consts:\n",
    "    del df_train[const]\n",
    "    del df_test[const]\n",
    "    \n",
    "consts = [col for col in df_macro.columns if len(df_macro[col].value_counts().index) == 1]\n",
    "for const in consts:\n",
    "    del df_macro[const]\n",
    "    \n",
    "# Low correlation with price #\n",
    "corr_limit = 0.1\n",
    "for column in df_train._get_numeric_data().columns.drop('price_doc').values:\n",
    "    if abs(df_train[column].corr(df_train['price_doc'])) < corr_limit:\n",
    "        df_train = df_train.drop(column, axis=1)\n",
    "        if column in df_test.columns.values:\n",
    "            df_test = df_test.drop(column, axis=1)\n",
    "            \n",
    "for column in df_macro._get_numeric_data().columns.values:\n",
    "    if abs(df_macro[column].corr(df_train['price_doc'])) < corr_limit:\n",
    "        df_macro = df_macro.drop(column, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity and Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VIF #\n",
    "# df_train[df_train==np.inf]=np.nan\n",
    "# df_train.fillna(df_train.mean(), inplace=True)\n",
    "categorial_ivs = set(df_train.columns) - set(df_train._get_numeric_data().columns)\n",
    "numeric_ivs = df_train._get_numeric_data().columns.drop('price_doc')\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(\n",
    "    df_train[numeric_ivs].values, i) for i in range(df_train[numeric_ivs].shape[1])]\n",
    "vif[\"features\"] = df_train[numeric_ivs].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raion_popul',\n",
       " 'young_all',\n",
       " 'work_all',\n",
       " 'ekder_all',\n",
       " 'cafe_count_3000',\n",
       " 'cafe_count_3000_na_price',\n",
       " 'cafe_count_3000_price_500',\n",
       " 'cafe_count_3000_price_1000',\n",
       " 'cafe_count_3000_price_1500',\n",
       " 'cafe_count_3000_price_2500',\n",
       " 'cafe_count_3000_price_4000',\n",
       " 'cafe_count_3000_price_high',\n",
       " 'cafe_count_5000',\n",
       " 'cafe_count_5000_na_price',\n",
       " 'cafe_count_5000_price_500',\n",
       " 'cafe_count_5000_price_1000',\n",
       " 'cafe_count_5000_price_1500',\n",
       " 'cafe_count_5000_price_2500',\n",
       " 'cafe_count_5000_price_4000',\n",
       " 'cafe_count_5000_price_high']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vif.loc[vif['VIF Factor'] == np.inf ].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features with VIF greater than 20\n",
    "\n",
    "features_to_remove = ['raion_popul',\n",
    " 'young_all',\n",
    " 'work_all',\n",
    " 'ekder_all',\n",
    " 'cafe_count_3000',\n",
    " 'cafe_count_3000_na_price',\n",
    " 'cafe_count_3000_price_500',\n",
    " 'cafe_count_3000_price_1000',\n",
    " 'cafe_count_3000_price_1500',\n",
    " 'cafe_count_3000_price_2500',\n",
    " 'cafe_count_3000_price_4000',\n",
    " 'cafe_count_3000_price_high',\n",
    " 'cafe_count_5000',\n",
    " 'cafe_count_5000_na_price',\n",
    " 'cafe_count_5000_price_500',\n",
    " 'cafe_count_5000_price_1000',\n",
    " 'cafe_count_5000_price_1500',\n",
    " 'cafe_count_5000_price_2500',\n",
    " 'cafe_count_5000_price_4000',\n",
    " 'cafe_count_5000_price_high']\n",
    "\n",
    "for f in features_to_remove:\n",
    "    if f in df_train:\n",
    "        del df_train[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df_train.columns.values)\n",
    "cols.pop(cols.index('price_doc'))\n",
    "df_train = df_train[cols + ['price_doc']]\n",
    "df_train.to_csv('../input/train_macro.csv', header=True, index=True)\n",
    "df_test.to_csv('../input/test_macro.csv', header=True, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
