{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature/Data Transformation\n",
    "- Outliers\n",
    "- New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Data Imputation\n",
    "- regression\n",
    "- Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dimensionality Reduction\n",
    "- Features with Bad or Constant Data\n",
    "- Multicollinearity and Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import scipy as sp\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrix\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('github'))))\n",
    "import utils.preprocessing as pp \n",
    "import utils.correlation as cr\n",
    "import utils.statsmodel_helper as st\n",
    "import utils.var_inflation_factor as vif\n",
    "\n",
    "df_macro = pd.read_csv('../input/macro.csv', parse_dates=['timestamp'])\n",
    "df_train = pd.read_csv('../input/train.csv', index_col=0, parse_dates=['timestamp'])\n",
    "df_test = pd.read_csv('../input/test.csv', index_col=0, parse_dates=['timestamp'])\n",
    "\n",
    "min_corr = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature/Data Transformation\n",
    "## outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop data with extremely big price #\n",
    "df_train = df_train.drop([2121]) \n",
    "\n",
    "# Replace outliers with proper value #\n",
    "df_train.loc[df_train.state == 33, 'state'] = 3\n",
    "df_train.loc[df_train['life_sq'] > 1000,     'life_sq']       = np.mean(df_train['life_sq'].dropna())\n",
    "df_train.loc[df_train['kitch_sq'] > 250,     'kitch_sq']      = np.mean(df_train['kitch_sq'].dropna())\n",
    "df_train.loc[df_train['num_room'] > 6,       'num_room']      = np.mean(df_train['num_room'].dropna())\n",
    "df_train.loc[df_train['build_year'] > 2017,  'build_year']    = np.mean(df_train['build_year'].dropna())\n",
    "df_train.loc[df_train['build_year'] < 1800,  'build_year']    = np.mean(df_train['build_year'].dropna())\n",
    "df_train.loc[df_train['floor'] > 50,         'floor']         = np.mean(df_train['floor'].dropna())\n",
    "df_train.loc[df_train['max_floor'] > 60,     'max_floor']     = np.mean(df_train['max_floor'].dropna())\n",
    "df_train.loc[df_train.full_sq == 0, 'full_sq'] = 50\n",
    "df_train = df_train[df_train.price_doc/df_train.full_sq <= 600000]\n",
    "df_train = df_train[df_train.price_doc/df_train.full_sq >= 10000]\n",
    "\n",
    "df_test.loc[df_test['life_sq'] > 1000,     'life_sq']       = np.mean(df_test['life_sq'].dropna())\n",
    "df_test.loc[df_test['kitch_sq'] > 250,     'kitch_sq']      = np.mean(df_test['kitch_sq'].dropna())\n",
    "df_test.loc[df_test['num_room'] > 6,       'num_room']      = np.mean(df_test['num_room'].dropna())\n",
    "df_test.loc[df_test['build_year'] > 2017,  'build_year']    = np.mean(df_test['build_year'].dropna())\n",
    "df_test.loc[df_test['build_year'] < 1800,  'build_year']    = np.mean(df_test['build_year'].dropna())\n",
    "df_test.loc[df_test['floor'] > 50,         'floor']         = np.mean(df_test['floor'].dropna())\n",
    "df_test.loc[df_test['max_floor'] > 60,     'max_floor']     = np.mean(df_test['max_floor'].dropna())\n",
    "df_test.loc[df_test.full_sq == 0, 'full_sq'] = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and day of week #\n",
    "df_train['month'] = df_train.timestamp.dt.month\n",
    "df_train['dow'] = df_train.timestamp.dt.dayofweek\n",
    "\n",
    "df_test['month'] = df_test.timestamp.dt.month\n",
    "df_test['dow'] = df_test.timestamp.dt.dayofweek\n",
    "\n",
    "df_train[\"yearweek\"] = df_train[\"timestamp\"].dt.year*100 + df_train[\"timestamp\"].dt.weekofyear\n",
    "df_test[\"yearweek\"] = df_test[\"timestamp\"].dt.year*100 + df_test[\"timestamp\"].dt.weekofyear\n",
    "\n",
    "# Create new features that might help #\n",
    "df_train['rel_floor'] = df_train['floor'] / df_train['max_floor'].astype(float)\n",
    "df_train['rel_kitch_sq'] = df_train['kitch_sq'] / df_train['full_sq'].astype(float)\n",
    "\n",
    "df_test['rel_floor'] = df_test['floor'] / df_test['max_floor'].astype(float)\n",
    "df_test['rel_kitch_sq'] = df_test['kitch_sq'] / df_test['full_sq'].astype(float)\n",
    "\n",
    "df_train.apartment_name=df_train.sub_area + df_train['metro_km_avto'].astype(str)\n",
    "df_test.apartment_name=df_test.sub_area + df_train['metro_km_avto'].astype(str)\n",
    "del df_train['metro_km_avto']\n",
    "del df_test['metro_km_avto']\n",
    "\n",
    "df_train['room_size'] = df_train['life_sq'] / df_train['num_room'].astype(float)\n",
    "df_test['room_size'] = df_test['life_sq'] / df_test['num_room'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average price corresponding to sub_area and ID_* #\n",
    "id_features = ['ID_metro',\n",
    "    'ID_railroad_station_walk', \\\n",
    "    'ID_big_road1', \\\n",
    "    'ID_big_road2', \\\n",
    "    'ID_railroad_terminal', \\\n",
    "    'ID_bus_terminal']\n",
    "\n",
    "for id_f in id_features:\n",
    "    df_test['avg_price_' + id_f] = 0.0\n",
    "    for val in df_test[id_f].unique():\n",
    "        if val == 171 and id_f == 'ID_metro':\n",
    "            df_test.loc[df_test.ID_metro == 171, 'avg_price_ID_metro'] = df_train[df_train.ID_metro == 170]['price_doc'].mean()\n",
    "            continue\n",
    "        if val == 132 and id_f == 'ID_railroad_station_walk':\n",
    "            df_test.loc[df_test.ID_railroad_station_walk == 132, 'avg_price_ID_railroad_station_walk'] = df_train[df_train.ID_railroad_station_walk == 131]['price_doc'].mean()\n",
    "            continue\n",
    "        if val == 121 and id_f == 'ID_railroad_station_walk':\n",
    "            df_test.loc[df_test.ID_railroad_station_walk == 122, 'avg_price_ID_railroad_station_walk'] = df_train[df_train.ID_railroad_station_walk == 131]['price_doc'].mean()\n",
    "            continue\n",
    "        avg = df_train[df_train[id_f] == val]['price_doc'].mean()\n",
    "        df_test.loc[df_test[id_f] == val, 'avg_price_' + id_f] = avg\n",
    "    del df_test[id_f]\n",
    "    \n",
    "for id_f in id_features:\n",
    "    df_train['avg_price_' + id_f] = 0.0\n",
    "    for val in df_train[id_f].unique():\n",
    "        avg = df_train[df_train[id_f] == val]['price_doc'].mean()\n",
    "        df_train.loc[df_train[id_f] == val, 'avg_price_' + id_f] = avg\n",
    "    del df_train[id_f]\n",
    "    \n",
    "cols = list(df_train.columns.values)\n",
    "cols.pop(cols.index('price_doc'))\n",
    "df_train = df_train[cols + ['price_doc']]\n",
    "\n",
    "\n",
    "df_test['avg_price_sub_area'] = 0.0\n",
    "df_train['avg_price_sub_area'] = 0.0\n",
    "for subarea in df_train['sub_area'].unique():\n",
    "    avg = df_train[df_train['sub_area'] == subarea]['price_doc'].mean()\n",
    "    df_train.loc[df_train['sub_area'] == subarea, 'avg_price_sub_area'] = avg\n",
    "    df_test.loc[df_test['sub_area'] == subarea, 'avg_price_sub_area'] = avg\n",
    "del df_train['sub_area']\n",
    "del df_test['sub_area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Data Imputation\n",
    "### regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_macro.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_train = pp.impute_by_regression(df_train, 4, 0.1)\n",
    "df_test = pp.impute_by_regression(df_test, 4, 0.1)\n",
    "df_macro = pp.impute_by_regression(df_macro, 4, 0.1)\n",
    "df_train._get_numeric_data()[df_train._get_numeric_data() < 0] = 0\n",
    "df_test._get_numeric_data()[df_test._get_numeric_data() < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_column</th>\n",
       "      <th>missing_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [missing_column, missing_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.find_missing_data_columns(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_column</th>\n",
       "      <th>missing_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>product_type</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   missing_column  missing_count\n",
       "10   product_type             33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.find_missing_data_columns(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_column</th>\n",
       "      <th>missing_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>child_on_acc_pre_school</td>\n",
       "      <td>658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>modern_education_share</td>\n",
       "      <td>1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>old_education_build_share</td>\n",
       "      <td>1389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               missing_column  missing_count\n",
       "78    child_on_acc_pre_school            658\n",
       "81     modern_education_share           1389\n",
       "82  old_education_build_share           1389"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.find_missing_data_columns(df_macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric #\n",
    "for col in df_train._get_numeric_data().columns[df_train._get_numeric_data().columns.isnull().any()].tolist():\n",
    "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
    "for col in df_test._get_numeric_data().columns[df_test._get_numeric_data().columns.isnull().any()].tolist():\n",
    "    df_test[col].fillna(df_train[col].mean(), inplace=True)\n",
    "\n",
    "# categorical #\n",
    "for col in df_train.columns[df_train.isnull().any()].tolist():\n",
    "    df_train[col].fillna(df_train[col].value_counts().index[0], inplace=True)\n",
    "for col in df_test.columns[df_test.isnull().any()].tolist():\n",
    "    df_test[col].fillna(df_train[col].value_counts().index[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dimensionality Reduction\n",
    "## Features with Bad or Constant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features with high correlation with other #\n",
    "features_to_remove = [\n",
    "    'children_preschool', 'children_school', 'male_f', \\\n",
    "    'female_f', 'young_male', 'young_female', 'work_male', \\\n",
    "    'work_female', 'ekder_male', 'ekder_female',\\\n",
    "    '0_6_all', '0_6_male', '0_6_female',\\\n",
    "    '7_14_all', '7_14_male', '7_14_female', '0_17_male', '0_17_female',\\\n",
    "    '16_29_male', '16_29_female', '0_13_all', '0_13_male', '0_13_female',\\\n",
    "]\n",
    "for f in features_to_remove:\n",
    "    del df_train[f]\n",
    "    del df_test[f]\n",
    "    \n",
    "# Macro features with bad data #\n",
    "del df_macro['modern_education_share']\n",
    "del df_macro['old_education_build_share']\n",
    "del df_macro['child_on_acc_pre_school']\n",
    "\n",
    "# Constant features #\n",
    "consts = [col for col in df_train.columns if len(df_train[col].value_counts().index) == 1]\n",
    "for const in consts:\n",
    "    del df_train[const]\n",
    "    del df_test[const]\n",
    "    \n",
    "consts = [col for col in df_macro.columns if len(df_macro[col].value_counts().index) == 1]\n",
    "for const in consts:\n",
    "    del df_macro[const]\n",
    "    \n",
    "# Low correlation with price #\n",
    "corr_limit = 0.1\n",
    "for column in df_train._get_numeric_data().columns.drop('price_doc').values:\n",
    "    if abs(df_train[column].corr(df_train['price_doc'])) < corr_limit:\n",
    "        df_train = df_train.drop(column, axis=1)\n",
    "        if column in df_test.columns.values:\n",
    "            df_test = df_test.drop(column, axis=1)\n",
    "\n",
    "corr_limit = 0.05           \n",
    "for column in df_macro._get_numeric_data().columns.values:\n",
    "    if abs(df_macro[column].corr(df_train['price_doc'])) < corr_limit:\n",
    "        df_macro = df_macro.drop(column, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity and Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[df_train==np.inf]=np.nan\n",
    "# df_train.fillna(df_train.median(), inplace=True)\n",
    "# categorial_ivs = set(df_train.columns.drop('timestamp')) - set(df_train._get_numeric_data().columns)\n",
    "# numeric_ivs = df_train._get_numeric_data().columns.drop('price_doc')\n",
    "# temp = vif.VarInflationFactor(impute=True, thresh=10.0).fit_transform(df_train[numeric_ivs])\n",
    "# df_train = pd.concat([df_train['timestamp'], temp, df_train[categorial_ivs], df_train['price_doc']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low correlation with price ; 0.1\n",
    "- Dropping raion_popul with vif=inf\n",
    "- Dropping cafe_count_3000 with vif=inf\n",
    "- Dropping cafe_count_5000 with vif=inf\n",
    "- Dropping cafe_avg_price_1500 with vif=14570698.395478534\n",
    "- Dropping raion_build_count_with_builddate_info with vif=307429.81510292145\n",
    "- Dropping kremlin_km with vif=84072.4496292116\n",
    "- Dropping cafe_count_2000 with vif=74874.23978052993\n",
    "- Dropping sadovoe_km with vif=41969.994861328116\n",
    "- Dropping cafe_count_1500 with vif=21039.17010239846\n",
    "- Dropping 0_17_all with vif=13646.574347445614\n",
    "- Dropping bulvar_ring_km with vif=6616.584984269324\n",
    "- Dropping cafe_sum_1500_max_price_avg with vif=1820.2389885218406\n",
    "- Dropping cafe_count_5000_price_1000 with vif=1740.0338127865925\n",
    "- Dropping yearweek with vif=1681.450093689153\n",
    "- Dropping school_km with vif=1335.81431222714\n",
    "- Dropping cafe_count_5000_price_1500 with vif=1147.081887683671\n",
    "- Dropping cafe_count_5000_price_2500 with vif=1029.403433436397\n",
    "- Dropping cafe_count_3000_price_1500 with vif=991.0424861610946\n",
    "- Dropping office_count_5000 with vif=897.5418693955999\n",
    "- Dropping cafe_count_1000 with vif=847.7716902197002\n",
    "- Dropping cafe_count_3000_price_500 with vif=762.3462932143118\n",
    "- Dropping office_count_3000 with vif=663.3000988405182\n",
    "- Dropping cafe_count_3000_price_2500 with vif=649.4909582035716\n",
    "- Dropping ttk_km with vif=623.0343503068661\n",
    "- Dropping cafe_count_2000_price_1500 with vif=575.0925791712715\n",
    "- Dropping cafe_count_2000_price_500 with vif=478.6132614851965\n",
    "- Dropping cafe_count_5000_price_500 with vif=456.50055580818736\n",
    "- Dropping avg_price_ID_railroad_terminal with vif=450.89525902835766\n",
    "- Dropping office_count_2000 with vif=392.24816093741754\n",
    "- Dropping church_count_5000 with vif=387.01978334666796\n",
    "- Dropping cafe_count_2000_price_2500 with vif=363.595326511891\n",
    "- Dropping cafe_count_3000_price_1000 with vif=320.1052674949561\n",
    "- Dropping cafe_count_1500_price_1500 with vif=282.746038549093\n",
    "- Dropping cafe_count_5000_na_price with vif=273.08773319959846\n",
    "- Dropping cafe_count_2000_price_1000 with vif=241.08785114367652\n",
    "- Dropping work_all with vif=210.87351861655605\n",
    "- Dropping zd_vokzaly_avto_km with vif=204.2106884705741\n",
    "- Dropping church_count_3000 with vif=195.53832783449283\n",
    "- Dropping oil_chemistry_km with vif=170.16364619582987\n",
    "- Dropping cafe_count_1500_price_500 with vif=161.52313340418294\n",
    "- Dropping cafe_count_5000_price_4000 with vif=161.05266399808895\n",
    "- Dropping avg_price_ID_bus_terminal with vif=160.64037784706218\n",
    "- Dropping cafe_count_3000_price_4000 with vif=152.6997344107902\n",
    "- Dropping office_count_1500 with vif=142.04934406000297\n",
    "- Dropping cafe_count_3000_na_price with vif=132.21990919926546\n",
    "- Dropping trc_count_5000 with vif=128.6064110563572\n",
    "- Dropping cafe_count_1500_price_1000 with vif=113.77495741603722\n",
    "- Dropping sport_count_5000 with vif=112.4224905925094\n",
    "- Dropping leisure_count_5000 with vif=111.13325674128215\n",
    "- Dropping avg_price_sub_area with vif=104.69793518191196\n",
    "- Dropping radiation_km with vif=102.64468640327081\n",
    "- Dropping big_church_count_3000 with vif=98.0565880785707\n",
    "- Dropping cafe_count_2000_na_price with vif=90.06136291617442\n",
    "- Dropping cafe_count_1500_price_2500 with vif=88.45223167705826\n",
    "- Dropping basketball_km with vif=88.18375546300162\n",
    "- Dropping avg_price_ID_big_road1 with vif=84.25375404590979\n",
    "- Dropping preschool_education_centers_raion with vif=83.51831699052997\n",
    "- Dropping green_part_3000 with vif=80.69687373913787\n",
    "- Dropping cafe_count_500 with vif=79.33669277723212\n",
    "- Dropping avg_price_ID_railroad_station_walk with vif=71.02406358359809\n",
    "- Dropping cafe_count_1000_price_1500 with vif=67.11836768927695\n",
    "- Dropping stadium_km with vif=62.725296129688836\n",
    "- Dropping avg_price_ID_big_road2 with vif=61.00583829565759\n",
    "- Dropping leisure_count_3000 with vif=59.1763655451112\n",
    "- Dropping preschool_km with vif=58.5367444833199\n",
    "- Dropping sport_count_3000 with vif=55.95603612412343\n",
    "- Dropping power_transmission_line_km with vif=53.51652710044996\n",
    "- Dropping office_sqm_3000 with vif=53.44907932128648\n",
    "- Dropping museum_km with vif=52.46393358334705\n",
    "- Dropping exhibition_km with vif=49.61405227768155\n",
    "- Dropping cafe_count_2000_price_high with vif=47.08872672928615\n",
    "- Dropping mosque_km with vif=45.106132716126076\n",
    "- Dropping workplaces_km with vif=42.99593357783127\n",
    "- Dropping cafe_count_1500_na_price with vif=41.018995362179574\n",
    "- Dropping university_km with vif=40.254959137620595\n",
    "- Dropping trc_count_3000 with vif=39.81452554144815\n",
    "- Dropping young_all with vif=38.17886789191855\n",
    "- Dropping cafe_sum_2000_max_price_avg with vif=37.695448912360646\n",
    "- Dropping office_count_1000 with vif=36.72871406591059\n",
    "- Dropping sport_count_2000 with vif=36.4826868577771\n",
    "- Dropping num_room with vif=35.68873158182139\n",
    "- Dropping office_sqm_5000 with vif=35.01348813441707\n",
    "- Dropping thermal_power_plant_km with vif=34.03738323300284\n",
    "- Dropping office_raion with vif=29.89884708835293\n",
    "- Dropping swim_pool_km with vif=29.71620942288434\n",
    "- Dropping railroad_station_avto_min with vif=28.87338406354351\n",
    "- Dropping green_part_5000 with vif=28.70960472644583\n",
    "- Dropping trc_count_2000 with vif=27.67002975966542\n",
    "- Dropping cafe_count_1000_price_1000 with vif=26.853821092056368\n",
    "- Dropping detention_facility_km with vif=26.05922781884997\n",
    "- Dropping big_church_km with vif=25.561778861496684\n",
    "- Dropping cafe_count_3000_price_high with vif=25.282559922207252\n",
    "- Dropping shopping_centers_km with vif=22.131764462314816\n",
    "- Dropping avg_price_ID_metro with vif=21.965512627738207\n",
    "- Dropping leisure_count_1500 with vif=21.95436798807885\n",
    "- Dropping nuclear_reactor_km with vif=21.12462602292889\n",
    "- Dropping cafe_sum_1500_min_price_avg with vif=20.6501108508169\n",
    "- Dropping office_sqm_2000 with vif=20.620193158569712\n",
    "- Dropping trc_sqm_5000 with vif=18.504195464013076\n",
    "- Dropping sport_objects_raion with vif=18.291767574631205\n",
    "- Dropping park_km with vif=18.053696961744084\n",
    "- Dropping full_sq with vif=18.0474213830802\n",
    "- Dropping big_road2_km with vif=17.408880983567126\n",
    "- Dropping ekder_all with vif=15.920877255305822\n",
    "- Dropping sport_count_1500 with vif=15.148369054142712\n",
    "- Dropping state with vif=13.91628839390911\n",
    "- Dropping public_healthcare_km with vif=13.856163768507596\n",
    "- Dropping ts_km with vif=13.674950877943553\n",
    "- Dropping big_church_count_5000 with vif=13.027726667821936\n",
    "- Dropping bus_terminal_avto_km with vif=12.75146274098609\n",
    "- Dropping theater_km with vif=12.495088881795647\n",
    "- Dropping area_m with vif=12.232469566632604\n",
    "- Dropping room_size with vif=12.030123165912915\n",
    "- Dropping raion_build_count_with_material_info with vif=11.589386861738545\n",
    "- Dropping cafe_count_1500_price_high with vif=11.431084074893445\n",
    "- Dropping office_sqm_1500 with vif=10.481504376675996\n",
    "- Dropping market_count_3000 with vif=10.194467884784672"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_removes = [\n",
    "    \"raion_popul\"\n",
    "    \"cafe_count_3000\" \n",
    "    \"cafe_count_5000\" \n",
    "    \"cafe_avg_price_1500\" \n",
    "    \"raion_build_count_with_builddate_info\" \n",
    "    \"kremlin_km\" \n",
    "    \"cafe_count_2000\" \n",
    "    \"sadovoe_km\" \n",
    "    \"cafe_count_1500\" \n",
    "    \"0_17_all\" \n",
    "    \"bulvar_ring_km\" \n",
    "    \"cafe_sum_1500_max_price_avg\" \n",
    "    \"cafe_count_5000_price_1000\" \n",
    "    \"yearweek\" \n",
    "    \"school_km\" \n",
    "    \"cafe_count_5000_price_1500\" \n",
    "    \"cafe_count_5000_price_2500\" \n",
    "    \"cafe_count_3000_price_1500\" \n",
    "    \"office_count_5000\" \n",
    "    \"cafe_count_1000\" \n",
    "    \"cafe_count_3000_price_500\" \n",
    "    \"office_count_3000\" \n",
    "    \"cafe_count_3000_price_2500\" \n",
    "    \"ttk_km\" \n",
    "    \"cafe_count_2000_price_1500\" \n",
    "    \"cafe_count_2000_price_500\" \n",
    "    \"cafe_count_5000_price_500\" \n",
    "    \"avg_price_ID_railroad_terminal\" \n",
    "    \"office_count_2000\" \n",
    "    \"church_count_5000\" \n",
    "    \"cafe_count_2000_price_2500\" \n",
    "    \"cafe_count_3000_price_1000\" \n",
    "    \"cafe_count_1500_price_1500\" \n",
    "    \"cafe_count_5000_na_price\" \n",
    "    \"cafe_count_2000_price_1000\" \n",
    "    \"work_all\" \n",
    "    \"zd_vokzaly_avto_km\" \n",
    "    \"church_count_3000\"\n",
    "    \"oil_chemistry_km\" \n",
    "    \"cafe_count_1500_price_500\" \n",
    "    \"cafe_count_5000_price_4000\" \n",
    "    \"avg_price_ID_bus_terminal\" \n",
    "    \"cafe_count_3000_price_4000\" \n",
    "    \"office_count_1500\"\n",
    "    \"cafe_count_3000_na_price\" \n",
    "    \"trc_count_5000\" \n",
    "    \"cafe_count_1500_price_1000\" \n",
    "    \"sport_count_5000\" \n",
    "    \"leisure_count_5000\"\n",
    "    \"avg_price_sub_area\"\n",
    "    \"radiation_km\" \n",
    "    \"big_church_count_3000\"\n",
    "    \"cafe_count_2000_na_price\"\n",
    "    \"cafe_count_1500_price_2500\"\n",
    "    \"basketball_km\" \n",
    "    \"avg_price_ID_big_road1\"\n",
    "    \"preschool_education_centers_raion\"\n",
    "    \"green_part_3000\" \n",
    "    \"cafe_count_500\" \n",
    "    \"avg_price_ID_railroad_station_walk\"\n",
    "    \"cafe_count_1000_price_1500\"\n",
    "    \"stadium_km\"\n",
    "    \"avg_price_ID_big_road2\" \n",
    "    \"leisure_count_3000\" \n",
    "    \"preschool_km\" \n",
    "    \"sport_count_3000\"\n",
    "    \"power_transmission_line_km\" \n",
    "    \"office_sqm_3000\" \n",
    "    \"museum_km\" \n",
    "    \"exhibition_km\"\n",
    "    \"cafe_count_2000_price_high\" \n",
    "    \"mosque_km\" \n",
    "    \"workplaces_km\"\n",
    "    \"cafe_count_1500_na_price\"\n",
    "    \"university_km\"\n",
    "    \"trc_count_3000\"\n",
    "    \"young_all\"\n",
    "    \"cafe_sum_2000_max_price_avg\" \n",
    "    \"office_count_1000\"\n",
    "    \"sport_count_2000\" \n",
    "    \"num_room\" \n",
    "    \"office_sqm_5000\" \n",
    "    \"thermal_power_plant_km\" \n",
    "    \"office_raion\" \n",
    "    \"swim_pool_km\" \n",
    "    \"railroad_station_avto_min\" \n",
    "    \"green_part_5000\" \n",
    "    \"trc_count_2000\" \n",
    "    \"cafe_count_1000_price_1000\"\n",
    "    \"detention_facility_km\"\n",
    "    \"big_church_km\" \n",
    "    \"cafe_count_3000_price_high\" \n",
    "    \"shopping_centers_km\"\n",
    "    \"avg_price_ID_metro\" \n",
    "    \"leisure_count_1500\" \n",
    "    \"nuclear_reactor_km\" \n",
    "    \"cafe_sum_1500_min_price_avg\" \n",
    "    \"office_sqm_2000\" \n",
    "    \"trc_sqm_5000\" \n",
    "    \"sport_objects_raion\" \n",
    "    \"park_km\" \n",
    "    \"full_sq\" \n",
    "    \"big_road2_km\"\n",
    "    \"ekder_all\" \n",
    "    \"sport_count_1500\"\n",
    "    \"state\" \n",
    "    \"public_healthcare_km\"\n",
    "    \"ts_km\" \n",
    "    \"big_church_count_5000\"\n",
    "    \"bus_terminal_avto_km\" \n",
    "    \"theater_km\" \n",
    "    \"area_m\"\n",
    "    \"room_size\" \n",
    "    \"raion_build_count_with_material_info\" \n",
    "    \"cafe_count_1500_price_high\"\n",
    "    \"office_sqm_1500\" \n",
    "    \"market_count_3000\" \n",
    "]\n",
    "\n",
    "for f in feature_to_removes:\n",
    "    if f in df_train:\n",
    "        del df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_macro = df_train.merge(df_macro, left_on='timestamp', right_on='timestamp', how='left').set_index(df_train.index)\n",
    "df_test_macro = df_test.merge(df_macro, left_on='timestamp', right_on='timestamp', how='left').set_index(df_test.index)\n",
    "cols = list(df_train_macro.columns.values)\n",
    "cols.pop(cols.index('price_doc'))\n",
    "df_train_macro = df_train_macro[cols + ['price_doc']]\n",
    "df_train_macro.to_csv('../input/train_macro.csv', header=True, index=True)\n",
    "df_test_macro.to_csv('../input/test_macro.csv', header=True, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
